
@article{aielloSensingTrendingTopics2013,
  title = {Sensing {{Trending Topics}} in {{Twitter}}},
  author = {Aiello, Luca Maria and Petkos, Georgios and Martin, Carlos and Corney, David and Papadopoulos, Symeon and Skraba, Ryan and G√∂ker, Ayse and Kompatsiaris, Ioannis and Jaimes, Alejandro},
  year = {2013},
  month = oct,
  journal = {IEEE Transactions on Multimedia},
  volume = {15},
  number = {6},
  pages = {1268--1282},
  issn = {1941-0077},
  doi = {10.1109/TMM.2013.2265080},
  abstract = {Online social and news media generate rich and timely information about real-world events of all kinds. However, the huge amount of data available, along with the breadth of the user base, requires a substantial effort of information filtering to successfully drill down to relevant topics and events. Trending topic detection is therefore a fundamental building block to monitor and summarize information originating from social sources. There are a wide variety of methods and variables and they greatly affect the quality of results. We compare six topic detection methods on three Twitter datasets related to major events, which differ in their time scale and topic churn rate. We observe how the nature of the event considered, the volume of activity over time, the sampling procedure and the pre-processing of the data all greatly affect the quality of detected topics, which also depends on the type of detection method used. We find that standard natural language processing techniques can perform well for social streams on very focused topics, but novel techniques designed to mine the temporal distribution of concepts are needed to handle more heterogeneous streams containing multiple stories evolving in parallel. One of the novel topic detection methods we propose, based on -grams cooccurrence and topic ranking, consistently achieves the best performance across all these conditions, thus being more reliable than other state-of-the-art techniques.},
  keywords = {Data mining,Information filtering,Media,Monitoring,Real-time systems,Sensors,social media,social sensing,text mining,topic detection,Twitter}
}

@inproceedings{arthurKmeansAdvantagesCareful2007,
  title = {K-Means++: The Advantages of Careful Seeding},
  shorttitle = {K-Means++},
  booktitle = {Proceedings of the Eighteenth Annual {{ACM-SIAM}} Symposium on {{Discrete}} Algorithms},
  author = {Arthur, David and Vassilvitskii, Sergei},
  year = {2007},
  month = jan,
  series = {{{SODA}} '07},
  pages = {1027--1035},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {{USA}},
  abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Œò(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
  isbn = {978-0-89871-624-5}
}

@article{baLayerNormalization2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  month = jul,
  journal = {arXiv:1607.06450 [cs, stat]},
  eprint = {1607.06450},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1607.06450},
  urldate = {2021-12-03},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{bleiLatentDirichletAllocation2003,
  title = {Latent {{Dirichlet Allocation}}},
  author = {Blei, David M and Ng, Andrew Y. and Jordan, Michael I.},
  year = {2003},
  month = mar,
  journal = {JMLR.org},
  volume = {3},
  pages = {993--1022},
  issn = {1532-4435},
  abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  langid = {english}
}

@incollection{bleiTopicModels2009,
  title = {Topic Models},
  booktitle = {Text Mining},
  author = {Blei, David M and Lafferty, John D},
  year = {2009},
  pages = {101--124},
  publisher = {{Chapman and Hall/CRC}},
  langid = {english}
}

@misc{bujokasCreatingWordEmbeddings2020,
  title = {Creating {{Word Embeddings}}: {{Coding}} the {{Word2Vec Algorithm}} in {{Python}} Using {{Deep Learning}}},
  shorttitle = {Creating {{Word Embeddings}}},
  author = {Bujokas, Eligijus},
  year = {2020},
  month = may,
  journal = {Medium},
  url = {https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8},
  urldate = {2021-12-02},
  abstract = {When I was writing another article that showcased how to use word embeddings in a text classification objective I realized that I always‚Ä¶},
  langid = {english},
  note = {Truy c·∫≠p l·∫ßn cu·ªëi 2021-12-02}
}

@article{camposYAKEKeywordExtraction2020,
  title = {{{YAKE}}! {{Keyword}} Extraction from Single Documents Using Multiple Local Features},
  author = {Campos, Ricardo and Mangaravite, V√≠tor and Pasquali, Arian and Jorge, Al√≠pio and Nunes, C√©lia and Jatowt, Adam},
  year = {2020},
  month = jan,
  journal = {Information Sciences},
  volume = {509},
  pages = {257--289},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2019.09.013},
  url = {https://www.sciencedirect.com/science/article/pii/S0020025519308588},
  urldate = {2022-01-25},
  abstract = {As the amount of generated information grows, reading and summarizing texts of large collections turns into a challenging task. Many documents do not come with descriptive terms, thus requiring humans to generate keywords on-the-fly. The need to automate this kind of task demands the development of keyword extraction systems with the ability to automatically identify keywords within the text. One approach is to resort to machine-learning algorithms. These, however, depend on large annotated text corpora, which are not always available. An alternative solution is to consider an unsupervised approach. In this article, we describe YAKE!, a light-weight unsupervised automatic keyword extraction method which rests on statistical text features extracted from single documents to select the most relevant keywords of a text. Our system does not need to be trained on a particular set of documents, nor does it depend on dictionaries, external corpora, text size, language, or domain. To demonstrate the merits and significance of YAKE!, we compare it against ten state-of-the-art unsupervised approaches and one supervised method. Experimental results carried out on top of twenty datasets show that YAKE! significantly outperforms other unsupervised methods on texts of different sizes, languages, and domains.},
  langid = {english},
  keywords = {Information extraction,Keyword extraction,Unsupervised Algorithm}
}

@article{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  journal = {arXiv:1810.04805 [cs]},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2021-10-25},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{dinhImprovingSocialTrend2021,
  title = {Improving {{Social Trend Detection Based}} on {{User Interaction}} and {{Combined}} with {{Keyphrase Extraction Using Text Features}} on {{Word Graph}}},
  booktitle = {Intelligent {{Systems}} and {{Networks}}},
  author = {Dinh, XuanTruong and Trinh, TienDat and Ngoc, TuyenDo and Pham, VanHai},
  editor = {Tran, Duc-Tan and Jeon, Gwanggil and Nguyen, Thi Dieu Linh and Lu, Joan and Xuan, Thu-Do},
  year = {2021},
  series = {Lecture {{Notes}} in {{Networks}} and {{Systems}}},
  pages = {163--170},
  publisher = {{Springer}},
  address = {{Singapore}},
  doi = {10.1007/978-981-16-2094-2_21},
  abstract = {Recently, by the explosion of information technology, the valuable and available data exponentially increases in various social media platforms which allow us to exploit and attain convenient information and transform it into knowledge. This means that prominent topics are extracted on time in the social media community by leveraging the proper techniques and methods. Although there are various novel approaches in this area, they almost ignore the factors of the user interactions. Besides, since the enormous size of the textual dataset is distributed to any languages and the requirements for trending detection in a specific language, most of the proposed methods concentrating on English. In this paper, we proposed a graph-based method for the Vietnamese dataset in which graph nodes represent posts. More specifically, the approach combines the user interactions with a word graph representation which then is extracted to the topic trends by the RankClus Algorithm. By applying our proposal in several Facebook and Twitter datasets, we introduce dominantly dependable and coherent results.},
  isbn = {9789811620942},
  langid = {english},
  keywords = {Keyphrase extraction,Social trending,Word graph}
}

@misc{ganegedaraIntuitiveGuideLatent2021,
  title = {Intuitive {{Guide}} to {{Latent Dirichlet Allocation}}},
  author = {Ganegedara, Thushan},
  year = {2021},
  month = nov,
  journal = {Medium},
  url = {https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158},
  urldate = {2021-12-01},
  abstract = {Topic modelling refers to the task of identifying topics that best describes a set of documents. These topics will only emerge during the‚Ä¶},
  langid = {english},
  note = {Truy c·∫≠p l·∫ßn cu·ªëi 2021-12-01}
}

@misc{GensimTopicModelling2021,
  title = {Gensim ‚Äì {{Topic Modelling}} in {{Python}}},
  year = {2021},
  month = dec,
  url = {https://github.com/RaRe-Technologies/gensim},
  urldate = {2021-12-04},
  abstract = {Topic Modelling for Humans},
  copyright = {LGPL-2.1},
  keywords = {data-mining,data-science,document-similarity,fasttext,gensim,information-retrieval,machine-learning,natural-language-processing,neural-network,nlp,python,topic-modeling,word-embeddings,word-similarity,word2vec}
}

@book{haightHandbookPoissonDistribution1967,
  title = {Handbook of the {{Poisson}} Distribution},
  author = {Haight, Frank A.},
  year = {1967},
  series = {Operations {{Research Society}} of {{America}}. {{Publications}} in Operations Research},
  number = {11},
  publisher = {{Wiley}},
  address = {{New York}},
  lccn = {QA273 .H32},
  keywords = {Poisson distribution,Stochastic processes}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {770--778},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
  urldate = {2021-12-03}
}

@misc{heidenreichPaperSummaryEvaluation2018,
  title = {Paper {{Summary}}: {{Evaluation}} of Sentence Embeddings in Downstream and Linguistic Probing Tasks},
  shorttitle = {Paper {{Summary}}},
  author = {Heidenreich, Hunter},
  year = {2018},
  month = aug,
  journal = {Medium},
  url = {https://towardsdatascience.com/paper-summary-evaluation-of-sentence-embeddings-in-downstream-and-linguistic-probing-tasks-5e6a8c63aab1},
  urldate = {2021-12-02},
  abstract = {Breaking down a paper that broke down some of the pros and cons of different sentence embeddings},
  langid = {english},
  note = {Truy c·∫≠p l·∫ßn cu·ªëi 2021-12-02}
}

@techreport{hendricksonTrendDetectionSocial2015,
  title = {Trend {{Detection}} in {{Social Data}}},
  author = {Hendrickson, Scott and Kolb, Jeff and Lehman, Briman and Montague, Joshua},
  year = {2015},
  month = jun,
  institution = {{GNIP}},
  note = {Published under Twitter and GNIP}
}

@book{jurafskySpeechLanguageProcessing2009,
  title = {Speech and {{Language Processing}}},
  author = {Jurafsky, Dan and Martin, James H.},
  year = {2009},
  edition = {3rd draft},
  publisher = {{Prentice Hall}},
  url = {https://web.stanford.edu/~jurafsky/slp3/},
  urldate = {2021-12-02}
}

@misc{jwtintelligenceFearMissingOut2015,
  title = {Fear {{Of Missing Out}}},
  author = {JWTIntelligence},
  year = {2015},
  month = jun,
  url = {https://web.archive.org/web/20150626125816/http://www.jwtintelligence.com/wp-content/uploads/2012/03/F_JWT_FOMO-update_3.21.12.pdf},
  urldate = {2021-11-25},
  langid = {english},
  note = {Truy c·∫≠p l·∫ßn cu·ªëi 2021-11-25}
}

@misc{kapadiaTopicModelingPython2020,
  title = {Topic {{Modeling}} in {{Python}}: {{Latent Dirichlet Allocation}} ({{LDA}})},
  shorttitle = {Topic {{Modeling}} in {{Python}}},
  author = {Kapadia, Shashank},
  year = {2020},
  month = dec,
  journal = {Medium},
  url = {https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0},
  urldate = {2021-12-01},
  abstract = {How to get started with topic modeling using LDA in Python},
  langid = {english},
  note = {Truy c·∫≠p l·∫ßn cu·ªëi 2021-12-01}
}

@misc{khanhBERTModel2020,
  title = {{BERT model}},
  shorttitle = {{BERT model}},
  author = {Kh√°nh, Ph·∫°m ƒê√¨nh},
  year = {2020},
  month = may,
  journal = {Khoa h·ªçc d·ªØ li·ªáu - Khanh's blog},
  url = {https://phamdinhkhanh.github.io/2020/05/23/BERTModel.html},
  urldate = {2021-12-03},
  langid = {vietnamese},
  note = {Truy c·∫≠p l·∫ßn cu·ªëi 2021-12-03}
}

@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1412.6980},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2022-05-21},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015}
}

@inproceedings{laffertyCorrelatedTopicModels2006,
  title = {Correlated {{Topic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lafferty, John and Blei, David},
  year = {2006},
  volume = {18},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper/2005/hash/9e82757e9a1c12cb710ad680db11f6f1-Abstract.html},
  urldate = {2021-12-05}
}

@phdthesis{lamGomCumVan2021,
  type = {Lu·∫≠n VƒÉn T·ªët Nghi·ªáp},
  title = {Gom C·ª•m VƒÉn B·∫£n D·ª±a Tr√™n M√¥ H√¨nh Ph√°t Hi·ªán Ch·ªß ƒê·ªÅ},
  author = {L√¢m, Nguy·ªÖn VƒÉn Quy·ªÅn},
  year = {2021},
  month = aug,
  school = {Tr∆∞·ªùng ƒê·∫°i h·ªçc B√°ch khoa - ƒêH Qu·ªëc gia TP.HCM}
}

@inproceedings{lauOnlineTrendAnalysis2012,
  title = {On-Line {{Trend Analysis}} with {{Topic Models}}: \#twitter {{Trends Detection Topic Model Online}}},
  shorttitle = {On-Line {{Trend Analysis}} with {{Topic Models}}},
  booktitle = {Proceedings of {{COLING}} 2012},
  author = {Lau, Jey Han and Collier, Nigel and Baldwin, Timothy},
  year = {2012},
  month = dec,
  pages = {1519--1534},
  publisher = {{The COLING 2012 Organizing Committee}},
  address = {{Mumbai, India}},
  url = {https://aclanthology.org/C12-1093},
  urldate = {2021-11-30}
}

@article{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.11692 [cs]},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1907.11692},
  urldate = {2021-12-04},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@article{madaniRealtimeTrendingTopics2015,
  title = {Real-Time Trending Topics Detection and Description from {{Twitter}} Content},
  author = {Madani, Amina and Boussaid, Omar and Zegour, Djamel Eddine},
  year = {2015},
  month = dec,
  journal = {Social Network Analysis and Mining},
  volume = {5},
  number = {1},
  pages = {59},
  issn = {1869-5450, 1869-5469},
  doi = {10.1007/s13278-015-0298-5},
  url = {http://link.springer.com/10.1007/s13278-015-0298-5},
  urldate = {2021-09-22},
  langid = {english}
}

@article{nguyenPhoBERTPretrainedLanguage2020,
  title = {{{PhoBERT}}: {{Pre-trained}} Language Models for {{Vietnamese}}},
  shorttitle = {{{PhoBERT}}},
  author = {Nguyen, Dat Quoc and Nguyen, Anh Tuan},
  year = {2020},
  month = oct,
  journal = {arXiv:2003.00744 [cs]},
  eprint = {2003.00744},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2003.00744},
  urldate = {2021-12-04},
  abstract = {We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and Natural language inference. We release PhoBERT to facilitate future research and downstream applications for Vietnamese NLP. Our PhoBERT models are available at https://github.com/VinAIResearch/PhoBERT},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  note = {Comment: EMNLP 2020 (Findings)}
}

@phdthesis{nikolovTrendNoTrend2012,
  type = {Lu·∫≠n VƒÉn T·ªët Nghi·ªáp},
  title = {Trend or No Trend : A Novel Nonparametric Method for Classifying Time Series},
  shorttitle = {Trend or No Trend},
  author = {Nikolov, Stanislav},
  year = {2012},
  url = {https://dspace.mit.edu/handle/1721.1/85399},
  urldate = {2021-11-29},
  abstract = {In supervised classification, one attempts to learn a model of how objects map to labels by selecting the best model from some model space. The choice of model space encodes assumptions about the problem. We propose a setting for model specification and selection in supervised learning based on a latent source model. In this setting, we specify the model by a small collection of unknown latent sources and posit that there is a stochastic model relating latent sources and observations. With this setting in mind, we propose a nonparametric classification method that is entirely unaware of the structure of these latent sources. Instead, our method relies on the data as a proxy for the unknown latent sources. We perform classification by computing the conditional class probabilities for an observation based on our stochastic model. This approach has an appealing and natural interpretation - that an observation belongs to a certain class if it sufficiently resembles other examples of that class. We extend this approach to the problem of online time series classification. In the binary case, we derive an estimator for online signal detection and an associated implementation that is simple, efficient, and scalable. We demonstrate the merit of our approach by applying it to the task of detecting trending topics on Twitter. Using a small sample of Tweets, our method can detect trends before Twitter does 79\% of the time, with a mean early advantage of 1.43 hours, while maintaining a 95\% true positive rate and a 4\% false positive rate. In addition, our method provides the flexibility to perform well under a variety of tradeoffs between types of error and relative detection time.},
  copyright = {M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  annotation = {Accepted: 2014-03-06T15:39:15Z}
}

@article{petersDeepContextualizedWord2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  month = mar,
  journal = {arXiv:1802.05365 [cs]},
  eprint = {1802.05365},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1802.05365},
  urldate = {2021-12-03},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: NAACL 2018. Originally posted to openreview 27 Oct 2017. v2 updated for NAACL camera ready}
}

@inproceedings{phuvipadawatBreakingNewsDetection2010,
  title = {Breaking {{News Detection}} and {{Tracking}} in {{Twitter}}},
  booktitle = {2010 {{IEEE}}/{{WIC}}/{{ACM International Conference}} on {{Web Intelligence}} and {{Intelligent Agent Technology}}},
  author = {Phuvipadawat, Swit and Murata, Tsuyoshi},
  year = {2010},
  month = aug,
  volume = {3},
  pages = {120--123},
  doi = {10.1109/WI-IAT.2010.205},
  abstract = {Twitter has been used as one of the communication channels for spreading breaking news. We propose a method to collect, group, rank and track breaking news in Twitter. Since short length messages make similarity comparison difficult, we boost scores on proper nouns to improve the grouping results. Each group is ranked based on popularity and reliability factors. Current detection method is limited to facts part of messages. We developed an application called ‚ÄúHotstream‚Äù based on the proposed method. Users can discover breaking news from the Twitter timeline. Each story is provided with the information of message originator, story development and activity chart. This provides a convenient way for people to follow breaking news and stay informed with real-time updates.},
  keywords = {Feature extraction,Indexing,Information Retrieval,Nominations and elections,Real time systems,Real-time text-mining,Reliability,Topic Detection and Tracking,Twitter,User-generated content}
}

@article{reimersSentenceBERTSentenceEmbeddings2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.10084 [cs]},
  eprint = {1908.10084},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1908.10084},
  urldate = {2021-12-02},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textasciitilde 65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: Published at EMNLP 2019}
}

@article{sanhDistilBERTDistilledVersion2020,
  title = {{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter},
  shorttitle = {{{DistilBERT}}, a Distilled Version of {{BERT}}},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  year = {2020},
  month = feb,
  journal = {arXiv:1910.01108 [cs]},
  eprint = {1910.01108},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1910.01108},
  urldate = {2021-12-04},
  abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: February 2020 - Revision: fix bug in evaluation metrics, updated metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS 2019}
}

@inproceedings{sankaranarayananTwitterStandNewsTweets2009,
  title = {{{TwitterStand}}: News in Tweets},
  shorttitle = {{{TwitterStand}}},
  booktitle = {Proceedings of the 17th {{ACM SIGSPATIAL International Conference}} on {{Advances}} in {{Geographic Information Systems}}},
  author = {Sankaranarayanan, Jagan and Samet, Hanan and Teitler, Benjamin E. and Lieberman, Michael D. and Sperling, Jon},
  year = {2009},
  month = nov,
  series = {{{GIS}} '09},
  pages = {42--51},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1653771.1653781},
  url = {https://doi.org/10.1145/1653771.1653781},
  urldate = {2022-05-13},
  abstract = {Twitter is an electronic medium that allows a large user populace to communicate with each other simultaneously. Inherent to Twitter is an asymmetrical relationship between friends and followers that provides an interesting social network like structure among the users of Twitter. Twitter messages, called tweets, are restricted to 140 characters and thus are usually very focused. We investigate the use of Twitter to build a news processing system, called TwitterStand, from Twitter tweets. The idea is to capture tweets that correspond to late breaking news. The result is analogous to a distributed news wire service. The difference is that the identities of the contributors/reporters are not known in advance and there may be many of them. Furthermore, tweets are not sent according to a schedule: they occur as news is happening, and tend to be noisy while usually arriving at a high throughput rate. Some of the issues addressed include removing the noise, determining tweet clusters of interest bearing in mind that the methods must be online, and determining the relevant locations associated with the tweets.},
  isbn = {978-1-60558-649-6},
  keywords = {geotagging,news,online clustering,Twitter}
}

@inproceedings{sunRankClusIntegratingClustering2009,
  title = {{{RankClus}}: Integrating Clustering with Ranking for Heterogeneous Information Network Analysis},
  shorttitle = {{{RankClus}}},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Extending Database Technology}}: {{Advances}} in {{Database Technology}}},
  author = {Sun, Yizhou and Han, Jiawei and Zhao, Peixiang and Yin, Zhijun and Cheng, Hong and Wu, Tianyi},
  year = {2009},
  month = mar,
  series = {{{EDBT}} '09},
  pages = {565--576},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1516360.1516426},
  url = {https://doi.org/10.1145/1516360.1516426},
  urldate = {2022-05-10},
  abstract = {As information networks become ubiquitous, extracting knowledge from information networks has become an important task. Both ranking and clustering can provide overall views on information network data, and each has been a hot topic by itself. However, ranking objects globally without considering which clusters they belong to often leads to dumb results, e.g., ranking database and computer architecture conferences together may not make much sense. Similarly, clustering a huge number of objects (e.g., thousands of authors) in one huge cluster without distinction is dull as well. In this paper, we address the problem of generating clusters for a specified type of objects, as well as ranking information for all types of objects based on these clusters in a multi-typed (i.e., heterogeneous) information network. A novel clustering framework called RankClus is proposed that directly generates clusters integrated with ranking. Based on initial K clusters, ranking is applied separately, which serves as a good measure for each cluster. Then, we use a mixture model to decompose each object into a K-dimensional vector, where each dimension is a component coefficient with respect to a cluster, which is measured by rank distribution. Objects then are reassigned to the nearest cluster under the new measure space to improve clustering. As a result, quality of clustering and ranking are mutually enhanced, which means that the clusters are getting more accurate and the ranking is getting more meaningful. Such a progressive refinement process iterates until little change can be made. Our experiment results show that RankClus can generate more accurate clusters and in a more efficient way than the state-of-the-art link-based clustering methods. Moreover, the clustering results with ranks can provide more informative views of data compared with traditional clustering.},
  isbn = {978-1-60558-422-5}
}

@inproceedings{syedFullTextAbstractExamining2017,
  title = {Full-{{Text}} or {{Abstract}}? {{Examining Topic Coherence Scores Using Latent Dirichlet Allocation}}},
  shorttitle = {Full-{{Text}} or {{Abstract}}?},
  booktitle = {2017 {{IEEE International Conference}} on {{Data Science}} and {{Advanced Analytics}} ({{DSAA}})},
  author = {Syed, Shaheen and Spruit, Marco},
  year = {2017},
  month = oct,
  pages = {165--174},
  doi = {10.1109/DSAA.2017.61},
  abstract = {This paper assesses topic coherence and human topic ranking of uncovered latent topics from scientific publications when utilizing the topic model latent Dirichlet allocation (LDA) on abstract and full-text data. The coherence of a topic, used as a proxy for topic quality, is based on the distributional hypothesis that states that words with similar meaning tend to co-occur within a similar context. Although LDA has gained much attention from machine-learning researchers, most notably with its adaptations and extensions, little is known about the effects of different types of textual data on generated topics. Our research is the first to explore these practical effects and shows that document frequency, document word length, and vocabulary size have mixed practical effects on topic coherence and human topic ranking of LDA topics. We furthermore show that large document collections are less affected by incorrect or noise terms being part of the topic-word distributions, causing topics to be more coherent and ranked higher. Differences between abstract and full-text data are more apparent within small document collections, with differences as large as 90\% high-quality topics for full-text data, compared to 50\% high-quality topics for abstract data.},
  keywords = {Abstract,Adaptation models,Coherence,Data models,Full-Text,Latent Dirichlet Allocation,Probabilistic logic,Resource management,Semantics,Topic Coherence,Vocabulary}
}

@article{tehHierarchicalDirichletProcesses2012,
  title = {Hierarchical {{Dirichlet Processes}}},
  author = {Teh, Yee Whye and Jordan, Michael I. and Beal, Matthew J. and Blei, David M.},
  year = {2012},
  month = jan,
  journal = {Journal of the American Statistical Association},
  publisher = {{Taylor \& Francis}},
  doi = {10.1198/016214506000000302},
  url = {https://www.tandfonline.com/doi/abs/10.1198/016214506000000302},
  urldate = {2022-01-06},
  abstract = {We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume th...},
  copyright = {¬© American Statistical Association},
  langid = {english}
}

@misc{tomarTopicModelingUsing2019,
  title = {Topic Modeling Using {{Latent Dirichlet Allocation}}({{LDA}}) and {{Gibbs Sampling}} Explained!},
  author = {Tomar, Ankur},
  year = {2019},
  month = jul,
  journal = {Analytics Vidhya},
  url = {https://medium.com/analytics-vidhya/topic-modeling-using-lda-and-gibbs-sampling-explained-49d49b3d1045},
  urldate = {2021-12-01},
  abstract = {How Topic Modelling works and how to implement it using LDA and Gibbs Sampling},
  langid = {english}
}

@article{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  journal = {arXiv:1706.03762 [cs]},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2021-10-28},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{vuVnCoreNLPVietnameseNatural2018,
  title = {{{VnCoreNLP}}: {{A Vietnamese Natural Language Processing Toolkit}}},
  shorttitle = {{{VnCoreNLP}}},
  author = {Vu, Thanh and Nguyen, Dat Quoc and Nguyen, Dai Quoc and Dras, Mark and Johnson, Mark},
  year = {2018},
  journal = {Proceedings of the 2018 Conference of the North American Chapter of           the Association for Computational Linguistics: Demonstrations},
  eprint = {1801.01331},
  eprinttype = {arxiv},
  pages = {56--60},
  doi = {10.18653/v1/N18-5012},
  url = {http://arxiv.org/abs/1801.01331},
  urldate = {2021-12-04},
  abstract = {We present an easy-to-use and fast toolkit, namely VnCoreNLP---a Java NLP annotation pipeline for Vietnamese. Our VnCoreNLP supports key natural language processing (NLP) tasks including word segmentation, part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing, and obtains state-of-the-art (SOTA) results for these tasks. We release VnCoreNLP to provide rich linguistic annotations to facilitate research work on Vietnamese NLP. Our VnCoreNLP is open-source and available at: https://github.com/vncorenlp/VnCoreNLP},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, NAACL 2018, to appear}
}

@misc{wolfTransformersStateoftheArtNatural2020,
  title = {Transformers: {{State-of-the-Art Natural Language Processing}}},
  shorttitle = {Transformers},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Perric and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  year = {2020},
  month = oct,
  pages = {38--45},
  url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
  urldate = {2021-12-04},
  abstract = {ü§ó Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.},
  copyright = {Apache-2.0}
}

@article{zhuAligningBooksMovies2015,
  title = {Aligning {{Books}} and {{Movies}}: {{Towards Story-like Visual Explanations}} by {{Watching Movies}} and {{Reading Books}}},
  shorttitle = {Aligning {{Books}} and {{Movies}}},
  author = {Zhu, Yukun and Kiros, Ryan and Zemel, Richard and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  year = {2015},
  month = jun,
  journal = {arXiv:1506.06724 [cs]},
  eprint = {1506.06724},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1506.06724},
  urldate = {2021-12-03},
  abstract = {Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. To align movies and books we exploit a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition}
}


