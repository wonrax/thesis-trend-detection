
@article{baLayerNormalization2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  month = jul,
  journal = {arXiv:1607.06450 [cs, stat]},
  eprint = {1607.06450},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1607.06450},
  urldate = {2021-12-03},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{bleiLatentDirichletAllocation2003,
  title = {Latent {{Dirichlet Allocation}}},
  author = {Blei, David M and Ng, Andrew Y. and Jordan, Michael I.},
  year = {2003},
  month = mar,
  journal = {JMLR.org},
  volume = {3},
  pages = {993--1022},
  issn = {1532-4435},
  abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  langid = {english}
}

@incollection{bleiTopicModels2009,
  title = {Topic Models},
  booktitle = {Text Mining},
  author = {Blei, David M and Lafferty, John D},
  year = {2009},
  pages = {101--124},
  publisher = {{Chapman and Hall/CRC}},
  langid = {english}
}

@misc{bujokasCreatingWordEmbeddings2020,
  title = {Creating {{Word Embeddings}}: Coding the {{Word2Vec Algorithm}} in {{Python}} Using {{Deep Learning}}},
  shorttitle = {Creating {{Word Embeddings}}},
  author = {Bujokas, Eligijus},
  year = {2020},
  month = may,
  journal = {Medium},
  url = {https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8},
  urldate = {2021-12-02},
  abstract = {When I was writing another article that showcased how to use word embeddings in a text classification objective I realized that I always‚Ä¶},
  langid = {english},
  note = {Truy c·∫≠p l·∫ßn cu·ªëi 2021-12-02}
}

@article{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: Pre-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  journal = {arXiv:1810.04805 [cs]},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2021-10-25},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{ganegedaraIntuitiveGuideLatent2021,
  title = {Intuitive {{Guide}} to {{Latent Dirichlet Allocation}}},
  author = {Ganegedara, Thushan},
  year = {2021},
  month = nov,
  journal = {Medium},
  url = {https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158},
  urldate = {2021-12-01},
  abstract = {Topic modelling refers to the task of identifying topics that best describes a set of documents. These topics will only emerge during the‚Ä¶},
  langid = {english},
  note = {Truy c·∫≠p l·∫ßn cu·ªëi 2021-12-01}
}

@misc{GensimTopicModelling2021,
  title = {Gensim ‚Äì {{Topic Modelling}} in {{Python}}},
  year = {2021},
  month = dec,
  url = {https://github.com/RaRe-Technologies/gensim},
  urldate = {2021-12-04},
  abstract = {Topic Modelling for Humans},
  copyright = {LGPL-2.1},
  keywords = {data-mining,data-science,document-similarity,fasttext,gensim,information-retrieval,machine-learning,natural-language-processing,neural-network,nlp,python,topic-modeling,word-embeddings,word-similarity,word2vec}
}

@book{haightHandbookPoissonDistribution1967,
  title = {Handbook of the {{Poisson}} Distribution},
  author = {Haight, Frank A.},
  year = {1967},
  series = {Operations {{Research Society}} of {{America}}. {{Publications}} in Operations Research},
  number = {11},
  publisher = {{Wiley}},
  address = {{New York}},
  lccn = {QA273 .H32},
  keywords = {Poisson distribution,Stochastic processes}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {770--778},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
  urldate = {2021-12-03}
}

@misc{heidenreichPaperSummaryEvaluation2018,
  title = {Paper {{Summary}}: Evaluation of Sentence Embeddings in Downstream and Linguistic Probing Tasks},
  shorttitle = {Paper {{Summary}}},
  author = {Heidenreich, Hunter},
  year = {2018},
  month = aug,
  journal = {Medium},
  url = {https://towardsdatascience.com/paper-summary-evaluation-of-sentence-embeddings-in-downstream-and-linguistic-probing-tasks-5e6a8c63aab1},
  urldate = {2021-12-02},
  abstract = {Breaking down a paper that broke down some of the pros and cons of different sentence embeddings},
  langid = {english},
  note = {Truy c·∫≠p l·∫ßn cu·ªëi 2021-12-02}
}

@article{hendricksonTrendDetectionSocial2015,
  title = {Trend {{Detection}} in {{Social Data}}},
  author = {Hendrickson, Scott and Kolb, Jeff and Lehman, Briman and Montague, Joshua},
  year = {2015},
  month = jun,
  note = {Published under Twitter and GNIP}
}

@book{jurafskySpeechLanguageProcessing,
  title = {Speech and {{Language Processing}}},
  author = {Jurafsky, Dan and Martin, James H.},
  edition = {3rd draft},
  url = {https://web.stanford.edu/~jurafsky/slp3/},
  urldate = {2021-12-02}
}

@misc{jwtintelligenceFearMissingOut2015,
  title = {Fear {{Of Missing Out}}},
  author = {JWTIntelligence},
  year = {2015},
  month = jun,
  url = {https://web.archive.org/web/20150626125816/http://www.jwtintelligence.com/wp-content/uploads/2012/03/F_JWT_FOMO-update_3.21.12.pdf},
  urldate = {2021-11-25},
  langid = {english},
  note = {Truy c·∫≠p l·∫ßn cu·ªëi 2021-11-25}
}

@misc{kapadiaTopicModelingPython2020,
  title = {Topic {{Modeling}} in {{Python}}: Latent {{Dirichlet Allocation}} ({{LDA}})},
  shorttitle = {Topic {{Modeling}} in {{Python}}},
  author = {Kapadia, Shashank},
  year = {2020},
  month = dec,
  journal = {Medium},
  url = {https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0},
  urldate = {2021-12-01},
  abstract = {How to get started with topic modeling using LDA in Python},
  langid = {english},
  note = {Truy c·∫≠p l·∫ßn cu·ªëi 2021-12-01}
}

@misc{khanhBERTModel2020,
  title = {{BERT model}},
  shorttitle = {{BERT model}},
  author = {Kh√°nh, Ph·∫°m ƒê√¨nh},
  year = {2020},
  month = may,
  journal = {Khoa h·ªçc d·ªØ li·ªáu - Khanh's blog},
  url = {https://phamdinhkhanh.github.io/2020/05/23/BERTModel.html},
  urldate = {2021-12-03},
  langid = {vietnamese},
  note = {Truy c·∫≠p l·∫ßn cu·ªëi 2021-12-03}
}

@phdthesis{lamGomCumVan2021,
  type = {Lu·∫≠n VƒÉn T·ªët Nghi·ªáp},
  title = {Gom C·ª•m VƒÉn B·∫£n D·ª±a Tr√™n M√¥ H√¨nh Ph√°t Hi·ªán Ch·ªß ƒê·ªÅ},
  author = {L√¢m, Nguy·ªÖn VƒÉn Quy·ªÅn},
  year = {2021},
  month = aug,
  school = {Tr∆∞·ªùng ƒê·∫°i h·ªçc B√°ch khoa - ƒêH Qu·ªëc gia TP.HCM}
}

@inproceedings{lauOnlineTrendAnalysis2012,
  title = {On-Line {{Trend Analysis}} with {{Topic Models}}: \#twitter {{Trends Detection Topic Model Online}}},
  shorttitle = {On-Line {{Trend Analysis}} with {{Topic Models}}},
  booktitle = {Proceedings of {{COLING}} 2012},
  author = {Lau, Jey Han and Collier, Nigel and Baldwin, Timothy},
  year = {2012},
  month = dec,
  pages = {1519--1534},
  publisher = {{The COLING 2012 Organizing Committee}},
  address = {{Mumbai, India}},
  url = {https://aclanthology.org/C12-1093},
  urldate = {2021-11-30}
}

@article{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: A {{Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.11692 [cs]},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1907.11692},
  urldate = {2021-12-04},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@article{madaniRealtimeTrendingTopics2015,
  title = {Real-Time Trending Topics Detection and Description from {{Twitter}} Content},
  author = {Madani, Amina and Boussaid, Omar and Zegour, Djamel Eddine},
  year = {2015},
  month = dec,
  journal = {Social Network Analysis and Mining},
  volume = {5},
  number = {1},
  pages = {59},
  issn = {1869-5450, 1869-5469},
  doi = {10.1007/s13278-015-0298-5},
  url = {http://link.springer.com/10.1007/s13278-015-0298-5},
  urldate = {2021-09-22},
  langid = {english}
}

@article{nguyenPhoBERTPretrainedLanguage2020,
  title = {{{PhoBERT}}: Pre-Trained Language Models for {{Vietnamese}}},
  shorttitle = {{{PhoBERT}}},
  author = {Nguyen, Dat Quoc and Nguyen, Anh Tuan},
  year = {2020},
  month = oct,
  journal = {arXiv:2003.00744 [cs]},
  eprint = {2003.00744},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2003.00744},
  urldate = {2021-12-04},
  abstract = {We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and Natural language inference. We release PhoBERT to facilitate future research and downstream applications for Vietnamese NLP. Our PhoBERT models are available at https://github.com/VinAIResearch/PhoBERT},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  note = {Comment: EMNLP 2020 (Findings)}
}

@phdthesis{nikolovTrendNoTrend2012,
  type = {Lu·∫≠n VƒÉn T·ªët Nghi·ªáp},
  title = {Trend or No Trend : A Novel Nonparametric Method for Classifying Time Series},
  shorttitle = {Trend or No Trend},
  author = {Nikolov, Stanislav},
  year = {2012},
  url = {https://dspace.mit.edu/handle/1721.1/85399},
  urldate = {2021-11-29},
  abstract = {In supervised classification, one attempts to learn a model of how objects map to labels by selecting the best model from some model space. The choice of model space encodes assumptions about the problem. We propose a setting for model specification and selection in supervised learning based on a latent source model. In this setting, we specify the model by a small collection of unknown latent sources and posit that there is a stochastic model relating latent sources and observations. With this setting in mind, we propose a nonparametric classification method that is entirely unaware of the structure of these latent sources. Instead, our method relies on the data as a proxy for the unknown latent sources. We perform classification by computing the conditional class probabilities for an observation based on our stochastic model. This approach has an appealing and natural interpretation - that an observation belongs to a certain class if it sufficiently resembles other examples of that class. We extend this approach to the problem of online time series classification. In the binary case, we derive an estimator for online signal detection and an associated implementation that is simple, efficient, and scalable. We demonstrate the merit of our approach by applying it to the task of detecting trending topics on Twitter. Using a small sample of Tweets, our method can detect trends before Twitter does 79\% of the time, with a mean early advantage of 1.43 hours, while maintaining a 95\% true positive rate and a 4\% false positive rate. In addition, our method provides the flexibility to perform well under a variety of tradeoffs between types of error and relative detection time.},
  copyright = {M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  annotation = {Accepted: 2014-03-06T15:39:15Z}
}

@article{petersDeepContextualizedWord2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  month = mar,
  journal = {arXiv:1802.05365 [cs]},
  eprint = {1802.05365},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1802.05365},
  urldate = {2021-12-03},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: NAACL 2018. Originally posted to openreview 27 Oct 2017. v2 updated for NAACL camera ready}
}

@article{reimersSentenceBERTSentenceEmbeddings2019,
  title = {Sentence-{{BERT}}: Sentence {{Embeddings}} Using {{Siamese BERT}}-{{Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.10084 [cs]},
  eprint = {1908.10084},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1908.10084},
  urldate = {2021-12-02},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textasciitilde 65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: Published at EMNLP 2019}
}

@article{sanhDistilBERTDistilledVersion2020,
  title = {{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter},
  shorttitle = {{{DistilBERT}}, a Distilled Version of {{BERT}}},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  year = {2020},
  month = feb,
  journal = {arXiv:1910.01108 [cs]},
  eprint = {1910.01108},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1910.01108},
  urldate = {2021-12-04},
  abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: February 2020 - Revision: fix bug in evaluation metrics, updated metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS 2019}
}

@misc{tomarTopicModelingUsing2019,
  title = {Topic Modeling Using {{Latent Dirichlet Allocation}}({{LDA}}) and {{Gibbs Sampling}} Explained!},
  author = {Tomar, Ankur},
  year = {2019},
  month = jul,
  journal = {Analytics Vidhya},
  url = {https://medium.com/analytics-vidhya/topic-modeling-using-lda-and-gibbs-sampling-explained-49d49b3d1045},
  urldate = {2021-12-01},
  abstract = {How Topic Modelling works and how to implement it using LDA and Gibbs Sampling},
  langid = {english}
}

@article{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  journal = {arXiv:1706.03762 [cs]},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2021-10-28},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{vuVnCoreNLPVietnameseNatural2018,
  title = {{{VnCoreNLP}}: A {{Vietnamese Natural Language Processing Toolkit}}},
  shorttitle = {{{VnCoreNLP}}},
  author = {Vu, Thanh and Nguyen, Dat Quoc and Nguyen, Dai Quoc and Dras, Mark and Johnson, Mark},
  year = {2018},
  journal = {Proceedings of the 2018 Conference of the North American Chapter of           the Association for Computational Linguistics: Demonstrations},
  eprint = {1801.01331},
  eprinttype = {arxiv},
  pages = {56--60},
  doi = {10.18653/v1/N18-5012},
  url = {http://arxiv.org/abs/1801.01331},
  urldate = {2021-12-04},
  abstract = {We present an easy-to-use and fast toolkit, namely VnCoreNLP---a Java NLP annotation pipeline for Vietnamese. Our VnCoreNLP supports key natural language processing (NLP) tasks including word segmentation, part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing, and obtains state-of-the-art (SOTA) results for these tasks. We release VnCoreNLP to provide rich linguistic annotations to facilitate research work on Vietnamese NLP. Our VnCoreNLP is open-source and available at: https://github.com/vncorenlp/VnCoreNLP},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, NAACL 2018, to appear}
}

@misc{wolfTransformersStateoftheArtNatural2020,
  title = {Transformers: State-of-the-{{Art Natural Language Processing}}},
  shorttitle = {Transformers},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Perric and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  year = {2020},
  month = oct,
  pages = {38--45},
  url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
  urldate = {2021-12-04},
  abstract = {ü§ó Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.},
  copyright = {Apache-2.0}
}

@article{zhuAligningBooksMovies2015,
  title = {Aligning {{Books}} and {{Movies}}: Towards {{Story}}-like {{Visual Explanations}} by {{Watching Movies}} and {{Reading Books}}},
  shorttitle = {Aligning {{Books}} and {{Movies}}},
  author = {Zhu, Yukun and Kiros, Ryan and Zemel, Richard and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  year = {2015},
  month = jun,
  journal = {arXiv:1506.06724 [cs]},
  eprint = {1506.06724},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1506.06724},
  urldate = {2021-12-03},
  abstract = {Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. To align movies and books we exploit a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition}
}


