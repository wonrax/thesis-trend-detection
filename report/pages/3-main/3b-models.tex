\section{Các mô hình}
\label{sec:models}
Ba mô hình chính được báo cáo này nhắc tới bao gồm mô hình phát hiện xu
hướng, mô hình phát hiện chủ đề và mô hình phát hiện chủ đề dựa trên ngữ nghĩa.
Mô hình phát hiện xu hướng được nghiên cứu là mô hình được phát triển bởi
Twitter~\cite{hendricksonTrendDetectionSocial2015}, bao gồm mô hình xác suất và
mô hình thiên dữ liệu (data-driven). Mô hình phát hiện chủ đề là mô hình khai
thác sự xuất hiện của các từ khoá trong các tập văn bản để tìm ra sự tương quan
giữa các từ khoá và sự ảnh hưởng của từ khoá đối với chủ đề của văn bản. Mô
hình phát hiện chủ đề mà báo cáo này sẽ tập trung nghiên cứu là mô hình Latent
Dirichlet Allication (LDA). Mô hình phát hiện chủ đề dựa trên ngữ nghĩa là một
mô hình phát hiện chủ đề nhưng được dựa trên ngữ nghĩa của văn bản, hay ngữ
cảnh mà văn bản đó biểu hiện để thực hiện gom cụm chủ đề. Mô hình ngữ nghĩa
BERT và PhoBERT là đối tượng nghiên cứu và là cơ sở cho thực nghiệm, kết hợp
với LDA để có thể sinh ra kết quả tốt hơn~\cite{lamGomCumVan2021}.

\subsection{Mô hình phát hiện xu hướng}
Tương tác của người dùng trên mạng xã hội luôn có sự liên quan nhất định nào đó
đến các sự kiện ở ngoài thế giới
thực~\cite{hendricksonTrendDetectionSocial2015}. Để phát hiện xu hướng, ta cần
phải trả lời các câu hỏi như: sự kiện đó bắt đầu từ khi nào? Mức độ thay đổi
phạm vi của sự kiện này là lớn hay nhỏ? Và chúng thay đổi như thế nào đối với
các sự kiện bình thường (typical) khác? Việc này không chỉ giúp phân biệt giữa
các sự kiện bình thường và các sự kiện bất thường (atypical) mà còn giúp so
sánh các sự kiện bất thường với
nhau~\cite{hendricksonTrendDetectionSocial2015}.

Twitter là mạng xã hội cho phép người dùng đăng các bài đăng, hay còn được gọi
là Tweet. Trong một Tweet, người dùng có thể thêm văn bản (giới hạn 280 ký tự)
hoặc hình ảnh. Mô hình của Twitter định lượng các hành vi người dùng bằng cách
đếm hashtag, mention hoặc liên kết (link) trong một khoảng thời gian cố định
(bucketed count). Khi một định lượng được thể hiện bằng một từ hoặc cụm từ, ta
có thể gọi từ hoặc cụm từ đó là \textit{chủ
đề}~\cite{hendricksonTrendDetectionSocial2015}.

Tuy nhiên, ta không thể biết trước được mức độ của sự thay đổi, hay thời gian
mà sự thay đổi đó sẽ xảy ra. Có sự kiện chỉ diễn ra trong một vài giây, có sự
kiện có thể diễn ra trong hàng năm. Hơn nữa, định lượng cho các sự kiện đó có
thể thay đổi từ một vài Tweet cho đến hàng triệu Tweet. Để phát triển một giải
thuật có thể đáp ứng được các kích thước dữ liệu trên phạm vi rộng lớn là việc
không hề đơn giản~\cite{hendricksonTrendDetectionSocial2015}.

Nhiều kỹ thuật dùng để phát hiện xu hướng đều định nghĩa một mô hình cơ sở
(background model), là mô hình có thể đại diện cho giả thuyết không (null
hypothesis), về cơ bản có nghĩa là \textit{không trend} (không là xu hướng).
Những thay đổi của các định lượng so với mô hình cơ sở (deviation) được tính
toán thành một hệ số (figure-of-merit) $\eta$. Giá trị $\eta$ càng lớn thì sự
thay đổi của định lượng so với mô hình cơ sở càng nhiều. Và một giá trị $\eta$
được định nghĩa trước có thể được sử dụng để chấp nhận hoặc từ chối giả thuyết
không~\cite{hendricksonTrendDetectionSocial2015}.

Một kỹ thuật khác có thể bao gồm thành phần cơ sở (non-trend, hay \textit{không
trend}) và thành phần giống trend (trend-like, hay \textit{đang là xu hướng}).
Khi đó, giá trị $\eta$ sẽ thể hiện cho việc dữ liệu sẽ giống \textit{trend} hơn
hay dữ liệu sẽ giống \textit{không trend}
hơn~\cite{hendricksonTrendDetectionSocial2015}.

\subsubsection{Mô hình xác xuất Poisson}

Phân phối Poisson là phân phối xác suất rời rạc, dùng để thể hiện xác suất số
lần một sự kiện nào đó xảy ra trong một khoảng thời gian nhất
định~\cite{haightHandbookPoissonDistribution1967}. Vì vậy, phân phối Poisson có
khả năng áp dụng cho việc định lượng dữ liệu trên mạng xã hội, hay nói cách
khác, ta có thể mặc định dữ liệu đó tuân theo phân phối
Poisson~\cite{hendricksonTrendDetectionSocial2015}.

Ví dụ, ta xét số lượng Tweet có chứa hashtag "\#covid19" trong một khoảng thời
gian nào đó. Số lượng đó có thể thay đổi theo tần suất đăng bài của người dùng
trong ngày.  Nhưng nếu ta bỏ qua sự thay đổi đó, ta có thể nói, số lượng Tweet
có chứa hashtag "\#covid19" tuân theo phân phối Poisson:
\[ P\left(c_{i} ; \nu\right)=\nu^{c_{i}} \cdot e^{-\nu} / c_{i} ! \]

với:
\begin{itemize}
	\item $c_{i}$ là số lượng Tweet có chứa hashtag "\#covid19" trong một khoảng
	thời gian nhất định.
	\item $P$ là xác suất xuất hiện các Tweet tương tự với số lượng $c_{i}$.
	\item $\nu$ là số lượng kỳ vọng các Tweet có chứa hashtag "\#covid19".
\end{itemize}

Vì ta không thể biết chính xác giá trị $\nu$, nên ta có thể lấy số lượng Tweet ở
khoảng thời gian trước đó (đã biết) là $c_{i - 1}$ để biểu diễn cho $\nu$. Ta có
hệ số $\eta$ tại một thời điểm nào đó là:
\[ c_{i}=\eta \cdot \mathrm{CI}(\alpha, \nu)+\nu \]

với:
\begin{itemize}
	\item $\nu=c_{i-1}$.
	\item $CI(\alpha, \nu)$ là khoảng tin cậy (confidence interval) cho phân
	phối Poisson với mean $\nu$ và độ tin cậy $\alpha$.
\end{itemize}

Khi đó, một số lượng Tweet $c_{i}$ được định nghĩa để có thể từ chối giả thuyết
khi:
\[
	c_{i}>=\eta_{c} \cdot \operatorname{CI}\left(\alpha, c_{i-1}\right)+c_{i-1}
\]

với $\eta_{c}$ và $\alpha$ là các tham số để điều chỉnh hiệu suất của thuật
toán~\cite{hendricksonTrendDetectionSocial2015}.

Mô hình trên khá đơn giản và chỉ yêu cầu một điểm dữ liệu cho mô hình cơ sở.
Tuy nhiên với lượng dữ liệu thay đổi liên tục và thay đổi trong phạm vi rộng,
ta không thể tìm được một cặp tham số $\eta$ và $\alpha$ tối ưu cho mọi loại dữ
liệu. Hơn nữa, dữ liệu thực tế thường không tuân theo phân phối Poisson với
mean chỉ đơn giản là một điểm dữ liệu. Để giải quyết vấn đề này, ta có thể tính
mean bằng giá trị trung bình của nhiều điểm dữ liệu trong lịch sử trong cùng
một sliding window. Tuy nhiên, như đã nói ở trên, việc chọn tham số cho mô hình
để tối ưu cho dữ liệu với mọi kích thước và hình dạng là không
thể~\cite{hendricksonTrendDetectionSocial2015}.

\subsubsection{Mô hình thiên dữ liệu (data-driven)}
Để giải quyết các vấn đề của mô hình Poisson,
\cite{hendricksonTrendDetectionSocial2015}~đề xuất phương pháp đơn giản hơn đó
là so sánh dữ liệu cần phân tích với các dữ liệu đã được gán nhãn. Trước tiên,
ta phải phân loại dữ liệu có sẵn thành \textit{đang trend} hoặc \textit{không
trend}. Sau đó, ta định nghĩa một hàm tính khoảng cách từ dữ liệu đã gán nhãn
và dữ liệu ta cần phân tích:
\[ d(r, s)=\sum_{i}^{N}\left(r_{i}-s_{i}\right)^{2} \]

với:
\begin{itemize}
	\item $r$ là dữ liệu dạng time series đã được gán nhãn.
	\item $s$ là dữ liệu dạng time series đang được phân tích.
	\item $r_{i}$ và $s_{i}$ là các giá trị của dữ liệu tại thời điểm $i$ trong
	tập dữ liệu $r$ và $s$ có độ dài $N$.
\end{itemize}

Nếu $r$ có độ dài lớn hơn $s$, ta lấy giá trị nhỏ nhất trong các khoảng cách
$d(r_s, s)$, với $r_s$ là tập con (sub-series) của $r$. Với hàm khoảng cách ở
trên, ta có thể tính trọng lượng (weight) bằng cách:
\[ W(r,s)=e^{-\lambda \cdot d(r,s)} \]

Tham số $\lambda$ là một hệ số để điều chỉnh mức độ quan trọng giữa các time
series khác nhau hoặc giống nhau. Ví dụ, giá trị $\lambda$ lớn cho ra $W$ rất
nhỏ kể cả khi khoảng cách $r$ và $s$ là rất
lớn~\cite{hendricksonTrendDetectionSocial2015}. Khi đó, $\eta$ sẽ được tính
bằng tỉ lệ:
\[
\eta(s)=\frac{\sum_{r \in R+} W(r, s)}{\sum_{r \in R-} W(r, s)}
\]

với $R+$ là tập dữ liệu gồm nhiều time series được gán nhãn \textit{trend} và
$R-$ là dữ liệu được gán nhãn \textit{không trend}. Giá trị $\eta$ càng cao thì
time series $s$ càng \textit{giống trend} và ngược lại.

Khó khăn chính khi hiện thực mô hình này là việc gán nhãn cho các dữ liệu
\textit{đang trend} và \textit{không trend}. Bên cạnh đó, để mô hình hoạt động
trên mọi kích thước dữ liệu, việc thực hiện biến đổi (transformation) trên các
tập dữ liệu là cần thiết. Các biến đổi có thể là~\cite{nikolovTrendNoTrend2012}:

\begin{itemize}
	\item \textbf{Chuẩn hoá đường cơ sở (Baseline Normalization)}: Quan sát cho
		thấy khá nhiều chủ đề \textit{không trend} có tần suất Tweet rất cao,
		bên cạnh đó cũng có rất nhiều chủ đề \textit{đang trend} lại có tần
		suất Tweet rất thấp. Ví dụ, chủ đề "người" có tần suất cơ sở (baseline
		rate) rất cao vì nó là một âm tiết phổ biến trong tiếng Việt. Ta có thể
		sử dụng Baseline Normalization để nhấn mạnh (emphasize) các điểm dữ
		liệu cao hơn đường cơ sở và giảm độ quan trọng của các điểm dữ liệu
		thấp hơn đường cơ sở.
	
	\item \textbf{Chuẩn hoá spike (Spike Normalization)}: Các chủ đề
		\textit{đang trend} và các chủ đề \textit{không trend} còn khác nhau ở
		độ lớn và tần suất của các điểm spike (điểm có giá trị lớn đột ngột).
		Vì vậy, Spike Normalization giúp nhấn mạnh các điểm spike và ngược lại.
	
	\item \textbf{Thang đo lôgarit (Logarithmic Scale)}: Ta thường không thể
		biết được cách thức mà một chủ đề được lan toả ra khắp cộng đồng, nhưng
		ta biết được đa số các quá trình phân nhánh (branching process) đều
		phát sinh theo cấp số mũ. Vì vậy, ta có thể sử dụng thang đo lôgarit để
		định lượng, qua đó có thể khai thác các đặc điểm này.

\end{itemize}

Hơn nữa, ta phải điều chỉnh các tham số như $\lambda$, độ dài time series $s$
và $r$, chọn các phương thức biến đổi (transformation) để đạt được kết quả tốt
nhất, vì nhiều trong số chúng có ảnh hưởng trực tiếp tới các chỉ số như
true-positive và false-positive~\cite{hendricksonTrendDetectionSocial2015}.

\subsection{Mô hình phát hiện chủ đề}

\subsection{Mô hình phân tích ngữ nghĩa}

\subsection{VnCoreNLP}
