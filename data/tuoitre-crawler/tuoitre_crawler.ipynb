{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tuoitre_crawler.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPffy8YYE72y"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from functools import reduce\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import unicodedata\n",
        "import threading"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOxaHK5UXe6i"
      },
      "source": [
        "### Object classes\n",
        "Classes that define an Article object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6spT4Iywn6YS"
      },
      "source": [
        "class Article:\n",
        "\n",
        "  def __init__(self, id, source, title=None, date=None, tags=[], author=None,\n",
        "               excerpt=None, content=None, url=None, comments=[],\n",
        "               category=None, likes=None):\n",
        "\n",
        "    self.id = id\n",
        "    self.title = title\n",
        "    self.date = date\n",
        "    self.source = source\n",
        "    self.author = author\n",
        "    self.excerpt = excerpt\n",
        "    self.content = content\n",
        "    self.url = url\n",
        "    self.comments = comments # List of Comment objects\n",
        "    self.tags = tags\n",
        "    self.category = category\n",
        "    self.likes = likes  # Like count\n",
        "  \n",
        "  def __str__(self):\n",
        "    return (\"ID: {}\\nTitle: {}\\nExcerpt: {}\\nDate: {}\\nAuthor: {}\\n\" \\\n",
        "        + \"Source: {}\\nCategory: {}\\nURL: {}\\nTags: {}\\n\" \\\n",
        "        + \"Likes: {}\\nComments: {}\\n-----\\n{}\") \\\n",
        "        .format(self.id, self.title, self.excerpt, self.date, self.author,\n",
        "                self.source, self.category, self.url, self.tags, self.likes,\n",
        "                len(self.comments), self.content)\n",
        "  \n",
        "  def __hash__(self):\n",
        "    return hash(self.source + self.id)\n",
        "      \n",
        "  def to_dict(self):\n",
        "\n",
        "    date = self.date.isoformat() if self.date else None\n",
        "    \n",
        "    comments = [comment.to_dict() for comment in self.comments]\n",
        "\n",
        "    _dict = self.__dict__.copy()\n",
        "\n",
        "    _dict.update(date=date, comments=comments)\n",
        "\n",
        "    return _dict"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrDbsRibaHF0"
      },
      "source": [
        "class Comment:\n",
        "  \"\"\"\n",
        "  User comment in an article.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, id, author=None, content=None, date=None, replies=[],\n",
        "               likes=None):\n",
        "    self.id = id\n",
        "    self.author = author\n",
        "    self.content = content\n",
        "    self.date = date\n",
        "    self.replies = replies  # list of Comment objects\n",
        "    self.likes = likes\n",
        "  \n",
        "  def __str__(self):\n",
        "    return \\\n",
        "      \"ID: {}\\nAuthor: {}\\nContent: {}\\nLikes: {}\\nDate: {}\\nReplies: {}\" \\\n",
        "        .format(self.id, self.author, self.content, self.likes,\n",
        "                self.date, len(self.replies))\n",
        "  \n",
        "  def __hash__(self):\n",
        "      return hash(self.id)\n",
        "      \n",
        "  def __getitem__(self, key):\n",
        "      return self.replies[key]\n",
        "  \n",
        "  def to_dict(self):\n",
        "\n",
        "      replies = [reply.to_dict() for reply in self.replies]\n",
        "      \n",
        "      date = self.date.isoformat() if self.date else None\n",
        "\n",
        "      _dict = self.__dict__.copy()\n",
        "\n",
        "      _dict.update(date=date, replies=replies)\n",
        "\n",
        "      return _dict"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9vKHoTqaHFx"
      },
      "source": [
        "### Crawl data\n",
        "Crawl article (title, content, tags...) and its user comments.\n",
        "Skip this section if only loading data from file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF_O39EAXqx0"
      },
      "source": [
        "#### Crawler class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNLY0WKFKStn"
      },
      "source": [
        "class TuoiTreCrawler:\n",
        "  \"\"\"\n",
        "  Crawl articles from TuoiTre news. Support crawling articles from\n",
        "  sub-categories (e.g \"the-thao\" as in \"https://tuoitre.vn/the-thao.htm\").\n",
        "  \"\"\"\n",
        "\n",
        "  BASE_URL = \"https://tuoitre.vn\"\n",
        "  API_URL = \"https://id.tuoitre.vn/api\"\n",
        "  LIKE_COUNT_URL = \"https://s1.tuoitre.vn/count-object.htm\"\n",
        "  SOURCE_NAME = \"Tuổi Trẻ\"\n",
        "  \n",
        "  class Category:\n",
        "    \"\"\"\n",
        "    Sub-categories ids extracted from the URL\n",
        "    (e.g. https://tuoitre.vn/timeline/11/trang-12.htm).\n",
        "    Not a comprehensive list. Add more as needed.\n",
        "    \n",
        "    To find the id of a category, find the value with the key \"category_id\"\n",
        "    in the HTML source. Or monitor network traffic when \"See more\" is pressed.\n",
        "    \"\"\"\n",
        "\n",
        "    MOI_NHAT = 0\n",
        "    THE_GIOI = 2\n",
        "    THOI_SU = 3\n",
        "    SUC_KHOE = 12\n",
        "    VAN_HOA = 200017\n",
        "    CONG_NGHE = 200029\n",
        "    THE_THAO = 1209\n",
        "    GIAO_DUC = 13\n",
        "\n",
        "  def __init__(self, category: Category=None, crawl_comment=True, delay=0.5,\n",
        "               skip_these=set(), newer_only=False):\n",
        "\n",
        "    self.category = category\n",
        "    self.crawl_comment = crawl_comment\n",
        "\n",
        "    # A set of article ids to skip crawling.\n",
        "    # Can be used to enlarge the existing data (skip the ones that already\n",
        "    # crawled)\n",
        "    self.skip_these = skip_these\n",
        "    # Only get the articles newer than the existing ones in self.skip_these\n",
        "    # regardless the limit.\n",
        "    self.newer_only = newer_only\n",
        "\n",
        "    # Amount of delay in seconds after each request\n",
        "    # (to avoid overloading the server).\n",
        "    self.delay = delay\n",
        "\n",
        "  def get_page_url(self, cursor: int):\n",
        "    \"\"\"\n",
        "    Return the URL of the newspaper indexes given the cursor.\n",
        "    \"\"\"\n",
        "\n",
        "    assert self.category is not None\n",
        "\n",
        "    return TuoiTreCrawler.BASE_URL + \"/timeline/{}/trang-{}.htm\" \\\n",
        "                                          .format(self.category, cursor)\n",
        "  \n",
        "  def get_id(self, url: str):\n",
        "    \"\"\"\n",
        "    Return the article ID given the URL.\n",
        "    \"\"\"\n",
        "\n",
        "    return url.split(\"/\")[-1].split(\".\")[0].split(\"-\")[-1]\n",
        "  \n",
        "  def normalize_unicode(self, unicode_str):\n",
        "    \"\"\"\n",
        "    Normalize unicode string (e.g. remove \\xa0 characters).\n",
        "    \"\"\"\n",
        "    return unicodedata.normalize(\"NFKC\", unicode_str)\n",
        "\n",
        "  def find_article_urls(self, url: str, limit=float(\"inf\")):\n",
        "    \"\"\"\n",
        "    Find articles in a page given its URL.\n",
        "    Return a set of URLs to the main articles.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Getting article URLs from\", url)\n",
        "    response = requests.get(url)\n",
        "    response_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    news_items = response_soup\\\n",
        "      .find_all(\"li\", class_=\"news-item\")\n",
        "\n",
        "    article_urls = set()\n",
        "\n",
        "    # Signal the caller to stop\n",
        "    stop = False\n",
        "\n",
        "    for item in news_items:\n",
        "\n",
        "      if limit and limit <= len(article_urls):\n",
        "        break\n",
        "    \n",
        "      a_tag = item.find(\"a\", recursive=False)\n",
        "      article_url = TuoiTreCrawler.BASE_URL + a_tag[\"href\"]\n",
        "\n",
        "      if self.get_id(article_url) not in self.skip_these:\n",
        "        article_urls.add(article_url)\n",
        "\n",
        "      elif self.newer_only:\n",
        "        stop = True\n",
        "        break\n",
        "    \n",
        "    return article_urls, stop\n",
        "\n",
        "  def crawl_article_urls(self, limit=15):\n",
        "    \"\"\"\n",
        "    Try to find as many articles as possible given the limit.\n",
        "    Return a set of article URLs.\n",
        "    \"\"\"\n",
        "\n",
        "    article_urls = set()\n",
        "    cursor = 1\n",
        "\n",
        "    try:\n",
        "      while len(article_urls) < limit:\n",
        "    \n",
        "        page_url = self.get_page_url(cursor)\n",
        "        \n",
        "        new_urls, stop = self.find_article_urls(page_url, limit - len(article_urls))\n",
        "        article_urls.update(new_urls)\n",
        "\n",
        "        if stop:\n",
        "          break\n",
        "        \n",
        "        print(\"Found\", len(article_urls), \"/\", limit, \"article URLs\")\n",
        "\n",
        "        cursor += 1\n",
        "\n",
        "        time.sleep(self.delay)\n",
        "    except Exception as e:\n",
        "      print(\"\\nError while getting article URLs:\", e, \"\\n\")\n",
        "      pass\n",
        "\n",
        "    return article_urls\n",
        "\n",
        "  def get_likes_count(self, id, likes=[None]):\n",
        "    \"\"\"\n",
        "    Get the number of likes an article has received given its id.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "\n",
        "      url = TuoiTreCrawler.LIKE_COUNT_URL + \"?newsId=\" + id\n",
        "\n",
        "      like_count = requests.get(url, headers={\"Origin\": \"https://tuoitre.vn\"})\n",
        "\n",
        "      likes[0] = int(like_count.text)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"\\nError while getting likes for article with id\", id, \":\", e, \"\\n\")\n",
        "\n",
        "    return int(like_count.text)\n",
        "\n",
        "  def get_article(self, url: str):\n",
        "    \"\"\"\n",
        "    Get an article given its URL.\n",
        "    Return an Article object.\n",
        "    \"\"\"\n",
        "\n",
        "    id = self.get_id(url)\n",
        "    \n",
        "    likes = [None]  # A list makes it easier to pass data across threads\n",
        "\n",
        "    get_like_thread = threading.Thread(target=self.get_likes_count, args=[id, likes])\n",
        "    get_like_thread.start()\n",
        "\n",
        "    response = requests.get(url)\n",
        "    response_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    source = TuoiTreCrawler.SOURCE_NAME\n",
        "\n",
        "    try:\n",
        "      title = response_soup.find(\"meta\", {\"property\": \"og:title\"})[\"content\"]\n",
        "      author = response_soup.find(\"meta\", {\"name\": \"author\"})[\"content\"]\n",
        "      excerpt = response_soup.find(\"meta\", {\"name\": \"description\"})[\"content\"]\n",
        "      category = response_soup.find(\"meta\", {\"property\": \"article:section\"})[\"content\"]\n",
        "\n",
        "      tags = response_soup.find(\"meta\", {\"name\": \"keywords\"})[\"content\"]\n",
        "      if tags:\n",
        "        tags = tags.split(\",\")\n",
        "\n",
        "      # Format 2021-11-13T13:02:00+07:00\n",
        "      date = response_soup.find(\"meta\", {\"name\": \"pubdate\"})[\"content\"]\n",
        "      date = date.split(\"+\")[0] + \"+0700\"\n",
        "      date = datetime.strptime(date, \"%Y-%m-%dT%H:%M:%S%z\")\n",
        "\n",
        "      paragraphs = response_soup.find(id=\"main-detail-body\") \\\n",
        "        .find_all(\"p\", recursive=False)\n",
        "      # Can be an empty string because some articles\n",
        "      # might have no textual content (e.g. video, infographic)\n",
        "      content = reduce(lambda value,\n",
        "                       p: value + p.get_text().strip() + \"\\n\", paragraphs, \"\")\n",
        "      content = self.normalize_unicode(content)\n",
        "    except Exception as e:\n",
        "      print(\"\\nError while getting article with id\", id, \":\", e)\n",
        "      print(\"The article could be an unexpected format (Infographic, video...).\")\n",
        "      return None\n",
        "    \n",
        "    get_like_thread.join()\n",
        "\n",
        "    return Article(id, source, title, date, tags, author, excerpt, content,\n",
        "                   url, category=category, likes=likes[0])\n",
        "\n",
        "  def json_to_comment(self, json_comment: dict):\n",
        "    \"\"\"\n",
        "    Convert a JSON comment to a Comment object.\n",
        "    \"\"\"\n",
        "\n",
        "    id = json_comment[\"id\"]\n",
        "    author = json_comment[\"sender_fullname\"]\n",
        "    content = BeautifulSoup(json_comment[\"content\"], \"html.parser\").text\n",
        "    likes = int(json_comment[\"likes\"])\n",
        "\n",
        "    # E.g. 2021-11-12T09:32:32\n",
        "    str_date = json_comment[\"created_date\"]\n",
        "    date = datetime.strptime(str_date, \"%Y-%m-%dT%H:%M:%S\")\n",
        "  \n",
        "    replies = []\n",
        "    if json_comment[\"child_comments\"] is not None:\n",
        "      replies = [self.json_to_comment(reply)\n",
        "                    for reply in json_comment[\"child_comments\"]]\n",
        "\n",
        "    return Comment(id, author, content, date, replies, likes)\n",
        "\n",
        "  def _crawl_comments(self, id, cursor=1, limit=20):\n",
        "    \"\"\"\n",
        "    Get comments of an article given its ID.\n",
        "    Return a set of Comment objects.\n",
        "    \"\"\"\n",
        "\n",
        "    api_url = self.API_URL + \"/getlist-comment.api?\"\n",
        "    \n",
        "    url = api_url + \"objId={}&pageindex={}&pagesize={}&objType=1&sort=1\" \\\n",
        "                          .format(id, cursor, limit)\n",
        "\n",
        "    response = requests.get(url)\n",
        "    response_json = response.json()\n",
        "    \n",
        "    assert response_json[\"Success\"] == True\n",
        "    \n",
        "    data = json.loads(response_json[\"Data\"])\n",
        "    \n",
        "    comments = []\n",
        "    \n",
        "    for comment in data:\n",
        "      comments.append(self.json_to_comment(comment))\n",
        "    \n",
        "    return comments\n",
        "    \n",
        "  def crawl_comments(self, id, limit=float(\"inf\"), thread_return=[None]):\n",
        "    \"\"\"\n",
        "    Get comments of an article given its ID.\n",
        "    Return a set of Comment objects.\n",
        "    \"\"\"\n",
        "\n",
        "    comments = []\n",
        "    cursor = 1\n",
        "\n",
        "    try:\n",
        "      while len(comments) < limit:\n",
        "        new_comments = self._crawl_comments(id, cursor)\n",
        "\n",
        "        time.sleep(self.delay)\n",
        "\n",
        "        # Reached the end of the comments\n",
        "        if len(new_comments) == 0:\n",
        "          break\n",
        "\n",
        "        comments += new_comments\n",
        "        cursor += 1\n",
        "\n",
        "    except AssertionError:\n",
        "      # No more comments\n",
        "      pass\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"\\nError while getting crawling comments of post id\", id, \":\", e, \"\\n\")\n",
        "      pass\n",
        "    \n",
        "    thread_return[0] = comments\n",
        "\n",
        "    return comments\n",
        "\n",
        "  def crawl_articles(self, limit):\n",
        "    \"\"\"\n",
        "    Crawl articles given the limit.\n",
        "    Return a list of Article objects.\n",
        "    \"\"\"\n",
        "\n",
        "    articles = []\n",
        "    article_urls = self.crawl_article_urls(limit)\n",
        "    loss = 0\n",
        "\n",
        "    print(\"Getting\", len(article_urls), \"articles...\")\n",
        "    try:\n",
        "      for url in tqdm(article_urls, mininterval=0.5):\n",
        "\n",
        "        get_comments_thread = None\n",
        "        comments = [None] # A list makes it easier to pass data across threads\n",
        "\n",
        "        if self.crawl_comment:\n",
        "\n",
        "          id = self.get_id(url)\n",
        "          get_comments_thread = threading.Thread(target=self.crawl_comments,\n",
        "                                  kwargs={'id': id, 'thread_return': comments})\n",
        "          get_comments_thread.start()\n",
        "\n",
        "        a = self.get_article(url)\n",
        "\n",
        "        if a:\n",
        "          articles.append(a)\n",
        "          if get_comments_thread:\n",
        "            get_comments_thread.join()\n",
        "            a.comments = comments[0]\n",
        "\n",
        "        else:\n",
        "          loss += 1\n",
        "\n",
        "        time.sleep(self.delay)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"Error while getting articles:\", e)\n",
        "    \n",
        "    print(\"\\nSuccess:\", len(articles), \"/\", LIMIT, \"\\tLoss:\", loss)\n",
        "\n",
        "    return articles"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iOW6afLcTfy"
      },
      "source": [
        "#### Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTkQozJdewfB",
        "outputId": "cde249dd-67dc-4fd0-f4ff-21dd035dd734"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL8KFbzefRkq"
      },
      "source": [
        "# File path to store or load the crawled data\n",
        "FILE_PATH = \"/content/drive/MyDrive/data/suc_khoe_articles.csv\" #@param {type:\"string\"}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y1lPyXQcPPE"
      },
      "source": [
        "# Limit the number of articles\n",
        "LIMIT =  1000#@param {type:\"integer\"}\n",
        "\n",
        "# Time delayed inbetween GET requests (to avoid overloading the news server)\n",
        "DELAY = 0.1 #@param {type:\"number\"}\n",
        "\n",
        "CATEGORY = TuoiTreCrawler.Category.SUC_KHOE"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6NV2tsPaHF7"
      },
      "source": [
        "#### Start crawling articles\n",
        "Start crawling from scratch.\n",
        "\n",
        "**For the need to enlarge the existing database, refer to the section \"Enlarge the existing data\" below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIdyt7uh4Ce7",
        "outputId": "1f920c2b-ec73-4f89-ab21-6b0bd77a4727"
      },
      "source": [
        "crawler = TuoiTreCrawler(category=CATEGORY,\n",
        "                         delay=DELAY, crawl_comment=True)\n",
        "\n",
        "articles = crawler.crawl_articles(limit=LIMIT)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-1.htm\n",
            "Found 15 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-2.htm\n",
            "Found 23 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-3.htm\n",
            "Found 38 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-4.htm\n",
            "Found 53 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-5.htm\n",
            "Found 68 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-6.htm\n",
            "Found 83 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-7.htm\n",
            "Found 98 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-8.htm\n",
            "Found 113 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-9.htm\n",
            "Found 128 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-10.htm\n",
            "Found 143 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-11.htm\n",
            "Found 158 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-12.htm\n",
            "Found 173 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-13.htm\n",
            "Found 188 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-14.htm\n",
            "Found 203 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-15.htm\n",
            "Found 218 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-16.htm\n",
            "Found 233 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-17.htm\n",
            "Found 248 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-18.htm\n",
            "Found 263 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-19.htm\n",
            "Found 278 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-20.htm\n",
            "Found 293 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-21.htm\n",
            "Found 308 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-22.htm\n",
            "Found 323 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-23.htm\n",
            "Found 338 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-24.htm\n",
            "Found 353 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-25.htm\n",
            "Found 368 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-26.htm\n",
            "Found 383 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-27.htm\n",
            "Found 398 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-28.htm\n",
            "Found 413 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-29.htm\n",
            "Found 428 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-30.htm\n",
            "Found 443 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-31.htm\n",
            "Found 458 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-32.htm\n",
            "Found 473 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-33.htm\n",
            "Found 488 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-34.htm\n",
            "Found 503 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-35.htm\n",
            "Found 518 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-36.htm\n",
            "Found 533 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-37.htm\n",
            "Found 548 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-38.htm\n",
            "Found 563 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-39.htm\n",
            "Found 578 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-40.htm\n",
            "Found 593 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-41.htm\n",
            "Found 608 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-42.htm\n",
            "Found 623 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-43.htm\n",
            "Found 638 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-44.htm\n",
            "Found 653 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-45.htm\n",
            "Found 668 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-46.htm\n",
            "Found 683 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-47.htm\n",
            "Found 698 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-48.htm\n",
            "Found 713 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-49.htm\n",
            "Found 728 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-50.htm\n",
            "Found 743 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-51.htm\n",
            "Found 758 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-52.htm\n",
            "Found 773 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-53.htm\n",
            "Found 788 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-54.htm\n",
            "Found 803 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-55.htm\n",
            "Found 818 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-56.htm\n",
            "Found 833 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-57.htm\n",
            "Found 848 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-58.htm\n",
            "Found 863 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-59.htm\n",
            "Found 878 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-60.htm\n",
            "Found 893 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-61.htm\n",
            "Found 908 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-62.htm\n",
            "Found 923 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-63.htm\n",
            "Found 938 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-64.htm\n",
            "Found 953 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-65.htm\n",
            "Found 968 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-66.htm\n",
            "Found 983 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-67.htm\n",
            "Found 998 / 1000 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-68.htm\n",
            "Found 1000 / 1000 article URLs\n",
            "Getting 1000 articles...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▍   | 646/1000 [20:34<08:50,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error while getting article with id 2021110513110478 : 'NoneType' object has no attribute 'find_all'\n",
            "The article could be an unexpected format (Infographic, video...).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|██████▉   | 696/1000 [22:16<09:46,  1.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error while getting article with id 20211002013543091 : 'NoneType' object has no attribute 'find_all'\n",
            "The article could be an unexpected format (Infographic, video...).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 722/1000 [23:06<07:39,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error while getting article with id 20210925192946811 : 'NoneType' object has no attribute 'find_all'\n",
            "The article could be an unexpected format (Infographic, video...).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▍  | 745/1000 [23:49<07:10,  1.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error while getting article with id 20210915202950649 : 'NoneType' object has no attribute 'find_all'\n",
            "The article could be an unexpected format (Infographic, video...).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [32:07<00:00,  1.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Success: 996 / 1000 \tLoss: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSGBeFUJadpz"
      },
      "source": [
        "##### Preview crawled data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRjqzbU3aHF9"
      },
      "source": [
        "Preview the first and the last article of the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks1gj5d1aHF-",
        "outputId": "08740e7a-37b2-49b5-edfa-bb44c20d3320"
      },
      "source": [
        "titles = [article.title for article in articles]\n",
        "\n",
        "# Print the preview of the set of the articles\n",
        "def print_list_preview(l: list):\n",
        "  \"\"\"\n",
        "  Print the first and the last item of the list\n",
        "  \"\"\"\n",
        "  \n",
        "  if len(l) < 1:\n",
        "    return\n",
        "\n",
        "  print(\"[\", 1, \"]\", l[0])\n",
        "  if len(l) < 2: return\n",
        "\n",
        "  print(\"...\")\n",
        "  print(\"[\", len(l), \"]\", l[-1])\n",
        "\n",
        "print_list_preview(titles)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1 ] 'Bão cytokine' tấn công phổi, tim, thận, gan... người trẻ F0\n",
            "...\n",
            "[ 996 ] Bác sĩ Lương Lễ Hoàng qua đời\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsfK6G0LaHF_"
      },
      "source": [
        "Preview the first article content in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtz8Jkkjk1ZO",
        "outputId": "a5a415cd-93bb-4945-f0ba-3af9b0617722"
      },
      "source": [
        "print(articles[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID: 20210919223447809\n",
            "Title: 'Bão cytokine' tấn công phổi, tim, thận, gan... người trẻ F0\n",
            "Excerpt: TTO - Theo thống kê của Trung tâm hồi sức Bệnh viện Bạch Mai đặt tại Bệnh viện dã chiến số 16 (quận 7, TP.HCM), với công suất 500 giường bệnh thì có đến 70% bệnh nhân COVID-19 mắc phải 'cơn bão cytokine'.\n",
            "Date: 2021-09-20 06:20:00+07:00\n",
            "Author: TUOI TRE ONLINE\n",
            "Source: Tuổi Trẻ\n",
            "Category: Sức khỏe\n",
            "URL: https://tuoitre.vn/bao-cytokine-tan-cong-phoi-tim-than-gan-nguoi-tre-f0-20210919223447809.htm\n",
            "Tags: ['bão cytokine', 'covid-19', 'f0', 'bệnh viện dã chiến', 'virus tấn công', 'bệnh nhân trẻ']\n",
            "Likes: 141\n",
            "Comments: 4\n",
            "-----\n",
            "\"Cơn bão cytokine\" là hiện tượng hệ miễn dịch cơ thể phản ứng quá mức, giải phóng ồ ạt cytokine gây viêm, khiến các cơ quan nội tạng suy kiệt. Người khỏe mạnh khi bị virus tấn công, hệ miễn dịch sẽ phản ứng, cơ thể tiết ra chất cytokine để ức chế virus. Nhưng ở một số người, đặc biệt là người trẻ tiết ra quá nhiều cytokine, gây ảnh hưởng đến phủ tạng.\n",
            "Người trẻ nhất là 17 tuổi\n",
            "Cách đây không lâu, TP.HCM đã tiếp nhận và điều trị cho phi công người Anh - \"bệnh nhân 91\", trải qua 90 ngày điều trị, bệnh nhân có xảy ra hiện tượng cơ thể phản ứng quá mức với COVID-19, tạo ra \"cơn bão cytokine\", tấn công cả tế bào lành. Đây là ca nặng nhất tại Việt Nam trong đợt dịch đầu tiên.\n",
            "Ông Đỗ Ngọc Sơn, phó giám đốc Trung tâm cấp cứu, phụ trách điều trị tại Trung tâm hồi sức Bệnh viện Bạch Mai, cho biết trong đợt dịch lần này, rất nhiều bệnh nhân trẻ tuổi mắc phải hội chứng \"cơn bão cytokine\". Nhóm tuổi dễ xảy ra hiện tượng cơn bão dao động từ 17 - 40.\n",
            "\"Cơn bão cytokine ở bệnh nhân COVID-19 có thể gặp ở mọi lứa tuổi, nhưng thường gặp nhất ở người trẻ, những người có đáp ứng miễn dịch rất mạnh. Ở những người lớn tuổi, có thể xảy ra nhưng mức độ nhẹ hơn\", ông Sơn cho hay.\n",
            "Ông Tô Lê Hưng, phó giám đốc Bệnh viện điều trị COVID-19 Củ Chi, cho biết giai đoạn cao điểm cuối tháng 7, đầu tháng 8 hầu như ngày nào bệnh viện cũng có bệnh nhân vào \"cơn bão cytokine\", có thời điểm lên đến gần 30 ca. Trung bình khi bệnh nhân rơi vào cơn bão, chỉ cần khoảng 1 ngày là sẽ diễn tiến đến nguy kịch, có nguy cơ tử vong nếu không được can thiệp kịp thời.\n",
            "\"Cơn bão cytokine\" xảy ra ở những người trẻ là chủ yếu do hệ miễn dịch mạnh, đối với bệnh nhân mắc cơn bão sẽ suy hô hấp rất nhanh. Trung bình chỉ cần nửa ngày đến 1 ngày bệnh nhân sẽ có nguy cơ tử vong.\n",
            "Hiện nay có 2 phương pháp để điều trị cho bệnh nhân COVID-19 có xảy ra \"cơn bão cytokine\" đó là dùng thuốc kháng viêm, ức chế miễn dịch và lọc máu liên tục cần đến nhân sự, máy móc, chi phí rất cao...\n",
            "\"Trong đợt dịch lần thứ 4 này, với biến thể Delta, nhiều bệnh nhân trẻ tuổi, thậm chí là những trẻ em từ 15 - 16 tuổi mắc hội chứng cơn bão cytokine phải lọc máu liên tục, hỗ trợ hô hấp để giành lại sự sống\", bác sĩ Hưng cho hay.\n",
            "Bệnh nhân đã tiêm ngừa vắc xin thì tỉ lệ diễn tiến nặng giảm đi rõ rệt, số lượng gặp \"cơn bão cytokine\" cũng ít hơn.\n",
            "Phát hiện sớm hạn chế tử vong\n",
            "Bác sĩ Sơn cho biết \"cơn bão cytokine\" sẽ gây ảnh hưởng đến tất cả các phủ tạng như phổi, tim, thận, gan... - đây là những cơ quan sẽ trực tiếp bị chất độc do \"cơn bão cytokine\" gây tê liệt các phủ tạng. Do vậy, điều trị cho bệnh nhân gặp rất nhiều khó khăn. Các biện pháp điều trị đặc hiệu hiện nay ở Việt Nam như lọc máu hấp thụ, dùng các kháng thể đơn dòng, thuốc kháng viêm, ức chế hệ miễn dịch...\n",
            "Hiện nay tại Trung tâm hồi sức Bệnh viện Bạch Mai, bệnh nhân sẽ được làm các xét nghiệm để chẩn đoán lâm sàng, phát hiện sớm \"cơn bão cytokine\". Khi được chẩn đoán sớm, các biện pháp điều trị có thể ngăn chặn được cơn bão hình thành.\n",
            "\"Phát hiện sớm và điều trị sớm vô cùng quan trọng để hạn chế việc hình thành cơn bão cytokine. Trong giai đoạn điều trị cho bệnh nhân COVID-19, khi gặp phải cơn bão bệnh nhân sẽ được điều trị hỗ trợ bằng cách thở máy, hỗ trợ về tuần hoàn... Rất nhiều hỗ trợ bệnh nhân cần phải được triển khai để duy trì sự sống cho họ\", bác sĩ Sơn nói.\n",
            "Đối với các bệnh nhân F0, điều quan trọng nhất là theo dõi, đánh giá thường xuyên tình trạng bệnh để có biện pháp xử lý kịp thời. Nhiều người trẻ thường chủ quan cho rằng trẻ tuổi sẽ không ảnh hưởng, do vậy không nhận thức được hết vai trò của theo dõi sức khỏe.\n",
            "Người bệnh cần phải thường xuyên theo dõi sự thay đổi bão hòa độ oxy trong máu từ khi mắc bệnh cho đến giai đoạn sau, nếu không khi cơn bão đến, thời gian vàng để điều trị sẽ qua mất, ảnh hưởng đến kết quả điều trị. Do vậy, người bệnh cần ý thức được mức độ quan trọng của việc theo dõi sức khỏe, sự tiến triển của bệnh, khi có dấu hiệu bất thường phải báo ngay cho các cơ quan y tế.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GASVjQ2aHGA"
      },
      "source": [
        "Preview comments from a random article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrJ1UTUuaHGA",
        "outputId": "6f553fe5-0d02-48c5-9afb-00092957b88c"
      },
      "source": [
        "comments = TuoiTreCrawler()._crawl_comments(id=\"202111131227554\", limit=2)\n",
        "print(comments[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID: 290c3b60-4515-11ec-8e3d-295feb285a8a\n",
            "Author: Nguyễn Ân\n",
            "Content: Cho nghỉ việc được rồi, ý thức tệ đến thế là cùng.\n",
            "Likes: 6\n",
            "Date: 2021-11-14 13:36:12\n",
            "Replies: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJVrpHJtaHGB"
      },
      "source": [
        "### Serialize, store and load\n",
        "Serialization and Deserialization.\n",
        "\n",
        "Format: CSV with JSON columns inbetween."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb36YOYBYsnR"
      },
      "source": [
        "#### Helper classes\n",
        "- **ArticleSerialization**: Serialize Article object to dictionary. Deserialize dictionary to Article object.\n",
        "- **FileStorage**: Read/Write serialized objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SPXM3FnaHGB"
      },
      "source": [
        "class ArticleSerialization:\n",
        "  \"\"\"\n",
        "  Serialize and deserialize Article object.\n",
        "  \"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def serialize(obj):\n",
        "    \"\"\"\n",
        "    Transform an object to a serializable dictionary.\n",
        "    Return a dictionary.\n",
        "    \"\"\"\n",
        "\n",
        "    article_dict = obj.to_dict()\n",
        "\n",
        "    # Store the comments and tags as JSON objects\n",
        "    article_dict[\"comments\"] = json.dumps([x.to_dict() for x in obj.comments],\n",
        "                                          ensure_ascii=False)\n",
        "\n",
        "    article_dict[\"tags\"] = json.dumps(obj.tags, ensure_ascii=False)\n",
        "\n",
        "    return article_dict\n",
        "\n",
        "  def json_to_comment(json_comment: dict):\n",
        "    comment_dict = json_comment.copy()\n",
        "\n",
        "    replies = []\n",
        "    if len(json_comment[\"replies\"]) > 0:\n",
        "      replies = [ArticleSerialization.json_to_comment(reply)\n",
        "                    for reply in comment_dict[\"replies\"]]\n",
        "\n",
        "    comment_dict[\"replies\"] = replies\n",
        "    comment_dict[\"likes\"] = int(comment_dict[\"likes\"])\n",
        "\n",
        "    # 2021-11-08T18:18:45\n",
        "    comment_dict[\"date\"] = datetime.strptime(comment_dict[\"date\"],\n",
        "                                              \"%Y-%m-%dT%H:%M:%S\")\n",
        "    return Comment(**comment_dict)\n",
        "\n",
        "  @staticmethod\n",
        "  def deserialize(_dict):\n",
        "    \"\"\"\n",
        "    Deserialize an object from a dictionary.\n",
        "    Return an Article object.\n",
        "    \"\"\"\n",
        "\n",
        "    obj_dict = _dict.copy()\n",
        "\n",
        "    comments = [ArticleSerialization.json_to_comment(comment)\n",
        "                  for comment in json.loads(obj_dict[\"comments\"])]\n",
        "      \n",
        "    tags = json.loads(obj_dict[\"tags\"])\n",
        "    \n",
        "    # 2021-11-08T18:18:45+07:00\n",
        "    date = datetime.strptime(obj_dict[\"date\"], \"%Y-%m-%dT%H:%M:%S%z\")\n",
        "\n",
        "    likes = int(obj_dict[\"likes\"]) if obj_dict[\"likes\"] is not '' else None\n",
        "\n",
        "    obj_dict.pop(\"comments\", None)\n",
        "    obj_dict.pop(\"tags\", None)\n",
        "    obj_dict.pop(\"date\", None)\n",
        "    obj_dict.pop(\"likes\", None)\n",
        "\n",
        "    return Article(**obj_dict, comments=comments, tags=tags, date=date,\n",
        "                   likes=likes)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usLXvCuHaHGC"
      },
      "source": [
        "class FileStorage:\n",
        "  \"\"\"\n",
        "  Store and retrieve objects to/from a file.\n",
        "  \"\"\"\n",
        "\n",
        "  ENCODING = \"utf-8-sig\"\n",
        "  NEW_LINE = \"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def store(objects, file_path, mode=\"w\", sort=True):\n",
        "    \"\"\"\n",
        "    Store Article objects to a file.\n",
        "    Return the file path.\n",
        "    \"\"\"\n",
        "    sorted_objects = objects\n",
        "\n",
        "    if sort:\n",
        "      sorted_objects = sorted(objects, key=lambda o: o.date)\n",
        "\n",
        "    with open(file_path, mode, newline=FileStorage.NEW_LINE,\n",
        "              encoding=FileStorage.ENCODING) as f:\n",
        "\n",
        "      writer = csv.DictWriter(f, fieldnames=sorted_objects[0].to_dict().keys())\n",
        "      \n",
        "      if \"a\" not in mode:\n",
        "        writer.writeheader()\n",
        "\n",
        "      for obj in sorted_objects:\n",
        "        writer.writerow(ArticleSerialization.serialize(obj))\n",
        "\n",
        "    return file_path\n",
        "\n",
        "  @staticmethod\n",
        "  def load(file_path):\n",
        "    \"\"\"\n",
        "    Load Article objects from a file.\n",
        "    Return a list of object.\n",
        "    \"\"\"\n",
        "\n",
        "    objects = []\n",
        "\n",
        "    with open(file_path, \"r\", newline=FileStorage.NEW_LINE,\n",
        "              encoding=FileStorage.ENCODING) as f:\n",
        "\n",
        "      reader = csv.DictReader(f)\n",
        "      \n",
        "      for row in reader:\n",
        "        objects.append(ArticleSerialization.deserialize(row))\n",
        "    \n",
        "    return objects"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UofJYeAZK6m"
      },
      "source": [
        "#### Store crawled data\n",
        "Skip this if only loading the existing file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MAoKvSPRaHGC",
        "outputId": "2acd0498-6afb-47ba-c1ef-455c039c6a99"
      },
      "source": [
        "# CAUTIOUS: This will remove the existing content of the file\n",
        "FileStorage.store(articles, FILE_PATH, mode=\"w\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/data/suc_khoe_articles.csv'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMBNVtvBaHGD"
      },
      "source": [
        "#### Load data from file and deserialize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ_5ZfwPaHGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35989933-4f12-4a44-d144-310183846597"
      },
      "source": [
        "loaded_articles = FileStorage.load(FILE_PATH)\n",
        "print(\"Size:\", len(loaded_articles), \"entries\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size: 1008 entries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqrfXEXmlFbJ"
      },
      "source": [
        "### Enlarge the existing data\n",
        "Attempt to crawl new articles and skip ones that are already stored.\n",
        "\n",
        "*Refer to the section above to load the data first before proceed.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRlGfRxjlvaH",
        "outputId": "2d18e201-8b70-4e7c-b53c-a930631d2863"
      },
      "source": [
        "crawled_ids = set([a.id for a in loaded_articles])\n",
        "\n",
        "# Limit the number of articles.\n",
        "LIMIT =  16#@param {type:\"integer\"}\n",
        "\n",
        "# Time delayed inbetween GET requests (to avoid overloading the news server)\n",
        "DELAY = 0.1 #@param {type:\"number\"}\n",
        "\n",
        "# Whether to get only the articles that are newer than the existing ones.\n",
        "NEWER_ONLY = False #@param {type:\"boolean\"}\n",
        "\n",
        "CATEGORY = TuoiTreCrawler.Category.SUC_KHOE\n",
        "\n",
        "suckhoe_crawler = TuoiTreCrawler(category=CATEGORY, delay=DELAY,\n",
        "                                 crawl_comment=True, skip_these=crawled_ids,\n",
        "                                 newer_only=NEWER_ONLY)\n",
        "\n",
        "new_articles = suckhoe_crawler.crawl_articles(limit=LIMIT)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-1.htm\n",
            "Found 0 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-2.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-3.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-4.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-5.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-6.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-7.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-8.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-9.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-10.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-11.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-12.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-13.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-14.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-15.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-16.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-17.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-18.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-19.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-20.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-21.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-22.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-23.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-24.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-25.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-26.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-27.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-28.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-29.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-30.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-31.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-32.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-33.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-34.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-35.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-36.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-37.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-38.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-39.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-40.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-41.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-42.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-43.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-44.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-45.htm\n",
            "Found 1 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-46.htm\n",
            "Found 2 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-47.htm\n",
            "Found 2 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-48.htm\n",
            "Found 2 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-49.htm\n",
            "Found 2 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-50.htm\n",
            "Found 2 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-51.htm\n",
            "Found 2 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-52.htm\n",
            "Found 2 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-53.htm\n",
            "Found 2 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-54.htm\n",
            "Found 2 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-55.htm\n",
            "Found 2 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-56.htm\n",
            "Found 2 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-57.htm\n",
            "Found 3 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-58.htm\n",
            "Found 3 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-59.htm\n",
            "Found 3 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-60.htm\n",
            "Found 3 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-61.htm\n",
            "Found 3 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-62.htm\n",
            "Found 3 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-63.htm\n",
            "Found 3 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-64.htm\n",
            "Found 3 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-65.htm\n",
            "Found 4 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-66.htm\n",
            "Found 4 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-67.htm\n",
            "Found 4 / 16 article URLs\n",
            "Getting article URLs from https://tuoitre.vn/timeline/12/trang-68.htm\n",
            "Found 16 / 16 article URLs\n",
            "Getting 16 articles...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 4/16 [00:07<00:20,  1.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error while getting article with id 20210915202950649 : 'NoneType' object has no attribute 'find_all'\n",
            "The article could be an unexpected format (Infographic, video...).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|██████▉   | 11/16 [00:20<00:08,  1.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error while getting article with id 20210925192946811 : 'NoneType' object has no attribute 'find_all'\n",
            "The article could be an unexpected format (Infographic, video...).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████▏ | 13/16 [00:22<00:04,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error while getting article with id 2021110513110478 : 'NoneType' object has no attribute 'find_all'\n",
            "The article could be an unexpected format (Infographic, video...).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:27<00:00,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error while getting article with id 20211002013543091 : 'NoneType' object has no attribute 'find_all'\n",
            "The article could be an unexpected format (Infographic, video...).\n",
            "\n",
            "Success: 12 / 16 \tLoss: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eBsfyK-kpRdC",
        "outputId": "d8812a74-7851-4308-ec44-e98fe59ab7a4"
      },
      "source": [
        "# Append crawled data to existing file\n",
        "FileStorage.store(new_articles, FILE_PATH, mode=\"a\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/data/suc_khoe_articles.csv'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv9BRNBG7SYr"
      },
      "source": [
        "### Sort the whole file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Sn3pKGbr7Pbz",
        "outputId": "4b1a9176-45e8-4656-b308-fdcd14617b6a"
      },
      "source": [
        "loaded_articles = FileStorage.load(FILE_PATH)\n",
        "FileStorage.store(loaded_articles, FILE_PATH, mode=\"w\", sort=True)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/data/suc_khoe_articles.csv'"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdYNMYcNlBTC"
      },
      "source": [
        "### Duplication test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqMOL_zIk4Av"
      },
      "source": [
        "assert len(loaded_articles) == len(set([a.id for a in loaded_articles]))"
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}