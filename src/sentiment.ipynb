{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>rate</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>√Åo bao ƒë·∫πp ·∫°!!</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuy·ªát v·ªùi !</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2day ao khong giong trong.</td>\n",
       "      <td>NEG</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M√πi th∆°m,b√¥i l√™n da m·ªÅm da.</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V·∫£i ƒë·∫πp, d√†y d·∫∑n.</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31455</th>\n",
       "      <td>Kh√¥ng ƒë√°ng ti·ªÅn.</td>\n",
       "      <td>NEG</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31456</th>\n",
       "      <td>Qu·∫ßn r·∫•t ƒë·∫πp.</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31457</th>\n",
       "      <td>H√†ng ƒë·∫πp ƒë√∫ng gi√° ti·ªÅn.</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31458</th>\n",
       "      <td>Ch·∫•t v·∫£i kh√° ·ªïn.</td>\n",
       "      <td>POS</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31459</th>\n",
       "      <td>√°o r·∫•t ok nh√© , v·∫£i m·ªãn , len cao c·ªï n√†y ph·ªëi ...</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31460 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment label  rate  \\\n",
       "0                                         √Åo bao ƒë·∫πp ·∫°!!   POS     5   \n",
       "1                                            Tuy·ªát v·ªùi !   POS     5   \n",
       "2                             2day ao khong giong trong.   NEG     1   \n",
       "3                            M√πi th∆°m,b√¥i l√™n da m·ªÅm da.   POS     5   \n",
       "4                                      V·∫£i ƒë·∫πp, d√†y d·∫∑n.   POS     5   \n",
       "...                                                  ...   ...   ...   \n",
       "31455                                   Kh√¥ng ƒë√°ng ti·ªÅn.   NEG     1   \n",
       "31456                                      Qu·∫ßn r·∫•t ƒë·∫πp.   POS     5   \n",
       "31457                            H√†ng ƒë·∫πp ƒë√∫ng gi√° ti·ªÅn.   POS     5   \n",
       "31458                                   Ch·∫•t v·∫£i kh√° ·ªïn.   POS     4   \n",
       "31459  √°o r·∫•t ok nh√© , v·∫£i m·ªãn , len cao c·ªï n√†y ph·ªëi ...   POS     5   \n",
       "\n",
       "      Unnamed: 3  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4            NaN  \n",
       "...          ...  \n",
       "31455        NaN  \n",
       "31456        NaN  \n",
       "31457        NaN  \n",
       "31458        NaN  \n",
       "31459        NaN  \n",
       "\n",
       "[31460 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(r\"C:\\Users\\hahuy\\OneDrive\\Work\\School\\Thesis_TrendDetection\\data\\ecomm-reviews\\reviews-ecommerce.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>rate</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>√Åo bao ƒë·∫πp ·∫°!!</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuy·ªát v·ªùi !</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2day ao khong giong trong.</td>\n",
       "      <td>NEG</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M√πi th∆°m,b√¥i l√™n da m·ªÅm da.</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V·∫£i ƒë·∫πp, d√†y d·∫∑n.</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31455</th>\n",
       "      <td>Kh√¥ng ƒë√°ng ti·ªÅn.</td>\n",
       "      <td>NEG</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31456</th>\n",
       "      <td>Qu·∫ßn r·∫•t ƒë·∫πp.</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31457</th>\n",
       "      <td>H√†ng ƒë·∫πp ƒë√∫ng gi√° ti·ªÅn.</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31458</th>\n",
       "      <td>Ch·∫•t v·∫£i kh√° ·ªïn.</td>\n",
       "      <td>POS</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31459</th>\n",
       "      <td>√°o r·∫•t ok nh√© , v·∫£i m·ªãn , len cao c·ªï n√†y ph·ªëi ...</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31460 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment label  rate  \\\n",
       "0                                         √Åo bao ƒë·∫πp ·∫°!!   POS     5   \n",
       "1                                            Tuy·ªát v·ªùi !   POS     5   \n",
       "2                             2day ao khong giong trong.   NEG     1   \n",
       "3                            M√πi th∆°m,b√¥i l√™n da m·ªÅm da.   POS     5   \n",
       "4                                      V·∫£i ƒë·∫πp, d√†y d·∫∑n.   POS     5   \n",
       "...                                                  ...   ...   ...   \n",
       "31455                                   Kh√¥ng ƒë√°ng ti·ªÅn.   NEG     1   \n",
       "31456                                      Qu·∫ßn r·∫•t ƒë·∫πp.   POS     5   \n",
       "31457                            H√†ng ƒë·∫πp ƒë√∫ng gi√° ti·ªÅn.   POS     5   \n",
       "31458                                   Ch·∫•t v·∫£i kh√° ·ªïn.   POS     4   \n",
       "31459  √°o r·∫•t ok nh√© , v·∫£i m·ªãn , len cao c·ªï n√†y ph·ªëi ...   POS     5   \n",
       "\n",
       "      Unnamed: 3  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4            NaN  \n",
       "...          ...  \n",
       "31455        NaN  \n",
       "31456        NaN  \n",
       "31457        NaN  \n",
       "31458        NaN  \n",
       "31459        NaN  \n",
       "\n",
       "[31460 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform preprocessing\n",
    "import re\n",
    "\n",
    "# TODO use a teen code list\n",
    "rep = {\"k\": \"kh√¥ng\", \"ko\": \"kh√¥ng\", \"ƒëc\": \"ƒë∆∞·ª£c\", \"dc\": \"ƒë∆∞·ª£c\", \"a\": \"anh\", \"e\": \"em\", \"v\": \"v·∫≠y\", \"vs\": \"v·ªõi\"} # define desired replacements here\n",
    "\n",
    "# use these three lines to do the replacement\n",
    "rep_ = dict((r\"\\b{}\\b\".format(k), v) for k, v in rep.items())\n",
    "pattern = re.compile(\"|\".join(rep_.keys()), flags=re.I)\n",
    "\n",
    "dataset[\"comment\"] = dataset[\"comment\"].apply(lambda text: pattern.sub(lambda m: rep[re.escape(m.group(0)).lower()], text))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21392,) (5349,) (4719,)\n",
      "['S·∫Ω ·ªßng h·ªô th√™müòçüòçüòç.' 'c·∫£m ∆°n shop, Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi.'\n",
      " 'N√≥i chung l√† v·ªõi gi√° ti·ªÅn b·ªè ra th√¨ ch·∫•t l∆∞·ª£ng nh∆∞ th·∫ø l√† t·∫°m ·ªïn.'\n",
      " 'ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn Shop ph·ª•c v·ª• r·∫•t t·ªët.'\n",
      " 'T√∫i zip b·∫°c, c√≥ g√≥i ch·ªëng ·∫©m, th√™m c√°i mu·ªóng n·ªØa, th√®mmmmmmm...c√≥ ai ng·ªù ƒë√≥ l√† c√°m, l√† c√°m nghen.'\n",
      " 'D√πng ƒë·ªÉ s√†o th√¨ ƒë∆∞·ª£c ch·ª© chi√™n r√°n kh√¥ng ƒë∆∞·ª£c v√¨ n√≥ ho√†n to√†n kh√¥ng ch·ªëng d√≠nh.'\n",
      " 'To l·∫Øm nhag.' 'M√†u kh√¥ng gi·ªëng nh∆∞ h√¨nh.'\n",
      " 'Th·ªùi gian giao h√†ng r·∫•t nhanh ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi M√¨nh cao 167cm m·∫∑c size S v·ª´a nh∆∞ng t·∫°i eo m√¨nh h∆°i nh·ªè n√™n c√≥ r∆°i r·ªông eo x√≠u hihi.'\n",
      " 'ƒê·ªì xinh l·∫Øm v·ªõi c·∫£ giao h√†ng nhanh n·ªØa üòò.'\n",
      " 'S·∫£n ph·∫©m kh√¥ng nh∆∞ mong ƒë·ª£i,k√©m...' 'Ch·∫•t v·∫£i n√≥ng , to√†n ch·∫•t poly.'\n",
      " 'C√≤n l·∫°i m·ªçi th·ª© y h√¨nh.' 'Gi√° l·∫°i si√™u r·∫ª n·ªØa ·∫°.'\n",
      " 'Shop ph·ª•c v·ª• r·∫•t t·ªët R·∫•t ƒë√°ng ti·ªÅn Th·ªùi gian giao h√†ng r·∫•t nhanh √Åo m·∫∑c form ƒë·∫πp, cotton nh·∫π v√† m√°t.'\n",
      " 'Ch·∫•t l∆∞·ª£ng gi√†y t·ªët.'\n",
      " 'H√¨nh in ngo√†i sau h∆°i nh·∫°t Shop ph·ª•c v·ª• kh√° t·ªët Th·ªùi gian giao h√†ng nhanh.'\n",
      " 'Con m√¨nh th√≠ch l·∫Øm.'\n",
      " 'S·∫£n ph·∫©m tuy·ªát v·ªùi, must-have c·ªßa c√°c ch·ªã em lu√¥n nha üíØüíØüíØ.'\n",
      " 'Shop ph·ª•c v·ª• r·∫•t t·ªët.'] ['POS' 'POS' 'POS' 'POS' 'NEU' 'NEU' 'NEU' 'NEG' 'POS' 'POS' 'NEG' 'NEG'\n",
      " 'POS' 'POS' 'POS' 'POS' 'NEU' 'POS' 'POS' 'POS']\n",
      "['Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi.' 'ƒê·∫∑t m√†u v√†ng th√¨ ra m√†u g√¨ v·∫≠y shop???'\n",
      " 'T√¥i kh√¥ng h·ªÅ nh·∫≠n ƒë∆∞·ª£c s·∫£n ph·∫©m n√†y.' '√Åo m·ªèng h∆°n ·∫£nh ch·ª•p.' 'Tuy√™Ã£t.'] ['POS' 'NEG' 'NEG' 'NEG' 'POS']\n",
      "['M√°y ch·ª•p r·∫•t ok.'\n",
      " 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn R·∫•t ƒë√°ng ti·ªÅn Th·ªùi gian giao h√†ng r·∫•t nhanh , y√™u shop.'\n",
      " 'Ch·∫•t l∆∞·ª£ng tam duoc.' 'Hang r√¢t t√¥t ·ªßng h·ªô d√†i d√†i.' 'V·∫£i c≈©ng ƒë·∫πp n·ªØa.'] ['POS' 'POS' 'NEG' 'POS' 'POS']\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(dataset[\"comment\"].values, dataset[\"label\"].values, test_size=0.15, random_state=42)\n",
    "train_texts, valid_texts, train_labels, valid_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=232)\n",
    "\n",
    "print(train_texts.shape, valid_texts.shape, test_labels.shape)\n",
    "print(train_texts[:5], train_labels[:5])\n",
    "print(valid_texts[:5], valid_labels[:5])\n",
    "print(test_texts[:5], test_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vncorenlp import VnCoreNLP\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", cache_dir=\"../phobert-base\")\n",
    "rdrsegmenter = VnCoreNLP(\n",
    "    \"./VnCoreNLP/VnCoreNLP-1.1.1.jar\",\n",
    "    annotators=\"wseg,pos\",\n",
    "    max_heap_size=\"-Xmx2g\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shop giao h√†ng nhanh nh∆∞ ch·ªõp lu√¥n .', 'ƒê√≥ng_g√≥i sp c·∫©n_th·∫≠n .', 'ƒê·ª£i test th·ª≠ v√†i ng√†y n·ªØa xem sao !', 'Shop ph·ª•c_v·ª• r·∫•t t·ªët . R·∫•t ƒë√°ng ti·ªÅn .', '√Åo h∆°i m·ªèng nh∆∞ng m·∫∑c ƒë·∫πp , d√¢y c·ªßa √°o ho√†n_thi·ªán kh√¥ng ƒë∆∞·ª£c t·ªët cho l·∫Øm , mong shop kh·∫Øc_ph·ª•c .'] ['Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi .', 'ƒê·∫∑t m√†u v√†ng th√¨ ra m√†u g√¨ v·∫≠y shop ? ? ?', 'T√¥i kh√¥ng h·ªÅ nh·∫≠n ƒë∆∞·ª£c s·∫£n_ph·∫©m n√†y .', '√Åo m·ªèng h∆°n ·∫£nh ch·ª•p .', 'Tuy√™Ã£t .'] ['M√°y ch·ª•p r·∫•t ok .', 'Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi ƒê√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn ƒê√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn R·∫•t ƒë√°ng ti·ªÅn Th·ªùi_gian giao h√†ng r·∫•t nhanh , y√™u shop .', 'Ch·∫•t_l∆∞·ª£ng tam duoc .', 'Hang r√¢t t√¥t ·ªßng_h·ªô d√†i_d√†i .', 'V·∫£i c≈©ng ƒë·∫πp n·ªØa .']\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA BEFORE FEEDING TO MODEL\n",
    "\n",
    "# Segmentize\n",
    "train_segmented = [rdrsegmenter.tokenize(text) for text in train_texts]\n",
    "valid_segmented = [rdrsegmenter.tokenize(text) for text in valid_texts]\n",
    "test_segmented = [rdrsegmenter.tokenize(text) for text in test_texts]\n",
    "\n",
    "# Flatten and merge back into strings\n",
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "train_segmented = [\" \".join(flatten(t)) for t in train_segmented]\n",
    "valid_segmented = [\" \".join(flatten(t)) for t in valid_segmented]\n",
    "test_segmented = [\" \".join(flatten(t)) for t in test_segmented]\n",
    "\n",
    "print(train_segmented[:5], valid_segmented[:5], test_segmented[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√≥ng_g√≥i sp c·∫©n_th·∫≠n .\n",
      "[0, 55662, 1685, 36150, 3289, 5, 2, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "train_encodings = tokenizer(train_segmented, truncation=True, padding=\"max_length\", max_length=256)\n",
    "valid_encodings = tokenizer(valid_segmented, truncation=True, padding=\"max_length\", max_length=256)\n",
    "test_encodings = tokenizer(test_segmented, truncation=True, padding=\"max_length\", max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê·ª£i test th·ª≠ v√†i ng√†y n·ªØa xem sao !\n",
      "[0, 15621, 13923, 1176, 515, 43, 348, 305, 423, 381, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_segmented[2])\n",
    "print(train_encodings[\"input_ids\"][2][:20])\n",
    "print(train_encodings[\"attention_mask\"][2][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 55662, 1685, 36150, 3289, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class EcommReviewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(set(labels))}\n",
    "        self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.label_to_idx[self.labels[idx]])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = EcommReviewsDataset(train_encodings, train_labels)\n",
    "val_dataset = EcommReviewsDataset(valid_encodings, valid_labels)\n",
    "test_dataset = EcommReviewsDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/vinai/phobert-base/resolve/main/config.json from cache at ../phobert-base\\a596f267f08b7158c7ab6300b1bf98eb6e1b05e6bcb0d7c18a8070364ee3011b.bbe27b2cac909b2279c83792c2d2b6f159f0a95f5d1c1eb66451da1c89a53609\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/vinai/phobert-base/resolve/main/pytorch_model.bin from cache at ../phobert-base\\8363542cfd9e2bad1a9a618e87ea1153d84819a3ae581cff0816a2c1f610f433.42a5e558f15db4cc3af338445707272b8f7545df78efdc125d3fd51025b22d85\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 188.00 MiB (GPU 0; 4.00 GiB total capacity; 2.92 GiB already allocated; 0 bytes free; 2.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21568/3181537405.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m trainer = Trainer(\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m                         \u001b[1;31m# the instantiated ü§ó Transformers model to be trained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[1;33m,\u001b[0m                  \u001b[1;31m# training arguments, defined above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hahuy\\code-projects\\school\\thesis-trend-detection\\env\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplace_model_on_device\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_move_model_to_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[1;31m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hahuy\\code-projects\\school\\thesis-trend-detection\\env\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_move_model_to_device\u001b[1;34m(self, model, device)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m         \u001b[1;31m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallel_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tie_weights\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hahuy\\code-projects\\school\\thesis-trend-detection\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32mc:\\Users\\hahuy\\code-projects\\school\\thesis-trend-detection\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hahuy\\code-projects\\school\\thesis-trend-detection\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hahuy\\code-projects\\school\\thesis-trend-detection\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hahuy\\code-projects\\school\\thesis-trend-detection\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    591\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hahuy\\code-projects\\school\\thesis-trend-detection\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[0;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[1;32m--> 897\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 0; 4.00 GiB total capacity; 2.92 GiB already allocated; 0 bytes free; 2.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=3, cache_dir=\"../phobert-base\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./sentiment-results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    per_device_eval_batch_size=1,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=len(dataset.label_to_ix), cache_dir=\"phobert-base\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e17fdab8067813884a193836e49f3945660fd5d00e539ef6cb2f60ea4da851c1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
