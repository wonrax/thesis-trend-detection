{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSz5jzj61nHc"
      },
      "source": [
        "This code is possible because of [Tae-Hwan Jung](https://github.com/graykode). I have just broken down the code and added few things here and here for better understanding. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-8kZmr4ItGUj"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import re\n",
        "from random import *\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "w6YMNvc8tbA9"
      },
      "outputs": [],
      "source": [
        "text = (\n",
        "    'Hello, how are you? I am Romeo.\\n'\n",
        "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
        "    'Nice meet you too. How are you today?\\n'\n",
        "    'Great. My baseball team won the competition.\\n'\n",
        "    'Oh Congratulations, Juliet\\n'\n",
        "    'Thanks you Romeo'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AhX8b1ydtrVf"
      },
      "outputs": [],
      "source": [
        "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
        "word_list = list(set(\" \".join(sentences).split()))\n",
        "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "\n",
        "\n",
        "for i, w in enumerate(word_list):\n",
        "    word_dict[w] = i + 4\n",
        "number_dict = {i: w for i, w in enumerate(word_dict)}\n",
        "vocab_size = len(word_dict)\n",
        "\n",
        "token_list = list()\n",
        "for sentence in sentences:\n",
        "    arr = [word_dict[s] for s in sentence.split()]\n",
        "    token_list.append(arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q03SGkfIu_Kd"
      },
      "outputs": [],
      "source": [
        "maxlen = 30 # maximum of length\n",
        "batch_size = 6\n",
        "max_pred = 5  # max tokens of prediction\n",
        "n_layers = 6 # number of Encoder of Encoder Layer\n",
        "n_heads = 12 # number of heads in Multi-Head Attention\n",
        "d_model = 768 # Embedding Size\n",
        "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
        "d_k = d_v = 64  # dimension of K(=Q), V\n",
        "n_segments = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TtyOOmRntu8w"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_batch():\n",
        "    batch = []\n",
        "    positive = negative = 0\n",
        "    while positive != batch_size/2 or negative != batch_size/2:\n",
        "        tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences))\n",
        "        tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
        "\n",
        "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
        "\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        #MASK LM\n",
        "        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # 15 % of tokens in one sentence\n",
        "\n",
        "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
        "                          if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
        "        shuffle(cand_maked_pos)\n",
        "        masked_tokens, masked_pos = [], []\n",
        "        for pos in cand_maked_pos[:n_pred]:\n",
        "            masked_pos.append(pos)\n",
        "            masked_tokens.append(input_ids[pos])\n",
        "            if random() < 0.8:  # 80%\n",
        "                input_ids[pos] = word_dict['[MASK]'] # make mask\n",
        "            elif random() < 0.5:  # 10%\n",
        "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
        "                input_ids[pos] = word_dict[number_dict[index]] # replace\n",
        "\n",
        "        # Zero Paddings\n",
        "        n_pad = maxlen - len(input_ids)\n",
        "        input_ids.extend([0] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "\n",
        "    #     # Zero Padding (100% - 15%) tokens\n",
        "        if max_pred > n_pred:\n",
        "            n_pad = max_pred - n_pred\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)\n",
        "\n",
        "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
        "            positive += 1\n",
        "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
        "            negative += 1\n",
        "    return batch\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s1PGksqBNuZM"
      },
      "outputs": [],
      "source": [
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lgJwW4OaiXE2"
      },
      "outputs": [],
      "source": [
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q7_HC-Y0jC3K"
      },
      "outputs": [],
      "source": [
        "batch = make_batch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XM1-FdPJi6p3"
      },
      "outputs": [],
      "source": [
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1, 25,  4, 11,  2,  3, 23, 21, 13,  3, 11,  5, 26,  6, 24,  2,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 1,  7, 10, 22, 24, 18,  3, 11,  2, 25,  4, 11,  2,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 1,  5,  6, 24,  3, 10, 22, 24, 27,  2,  7, 23, 21, 13, 14, 11,  3, 26,\n",
              "          6, 24,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 1,  7, 23, 21, 13, 14, 11,  3, 26,  3, 24,  2,  5,  6,  3, 19, 10, 22,\n",
              "         24, 27,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 1,  5,  6, 24, 19, 10, 22, 24, 27,  2,  8, 21,  3,  3, 28, 15, 17,  2,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 1,  3,  4, 11,  2, 12, 24, 23,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhM1DCU_iYCB",
        "outputId": "7525fdc7-f78e-488b-ef35-7d9ecfb969e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          ...,\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True]],\n",
              " \n",
              "         [[False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          ...,\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True]],\n",
              " \n",
              "         [[False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          ...,\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True]],\n",
              " \n",
              "         [[False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          ...,\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True]],\n",
              " \n",
              "         [[False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          ...,\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True]],\n",
              " \n",
              "         [[False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          ...,\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True],\n",
              "          [False, False, False,  ...,  True,  True,  True]]]),\n",
              " tensor([ 1, 25,  4, 11,  2,  3, 23, 21, 13,  3, 11,  5, 26,  6, 24,  2,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_attn_pad_mask(input_ids, input_ids), input_ids[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Qnay0LTDjE4S"
      },
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
        "        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
        "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        return self.norm(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rHjj-1wXjsdI"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context, attn "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X2rbGNMzl7o",
        "outputId": "81b17de0-c3ef-448e-896f-7143b39cbd62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scores:  tensor([-2.0049e-01, -1.7150e+00, -1.0340e+00,  1.1431e+00,  9.3378e-01,\n",
            "         7.7193e-01,  1.1849e+00, -9.6279e-01, -3.6691e-01,  5.3500e-01,\n",
            "         1.0898e-01,  1.1143e+00, -1.1070e+00,  1.4929e+00,  1.5531e-01,\n",
            "        -2.2383e-01, -1.1421e+00,  1.9107e+00, -1.9444e-01,  4.1770e-02,\n",
            "        -8.7240e-01, -8.6302e-01, -4.5168e-01,  2.1208e-01,  1.3907e+00,\n",
            "        -1.2553e+00,  4.5237e-02,  1.8089e-01, -1.0817e+00,  6.6788e-01,\n",
            "        -7.9101e-01,  2.2475e+00,  1.3092e+00,  1.7518e+00,  7.4958e-01,\n",
            "        -1.4086e+00, -9.5696e-01, -1.3003e+00,  4.9823e-01, -5.6824e-01,\n",
            "         2.3491e-01, -6.0092e-01,  8.0965e-01, -9.8654e-02, -4.5294e-01,\n",
            "        -2.0602e+00,  6.7350e-01, -2.6437e-01, -1.4832e+00,  7.4316e-01,\n",
            "        -7.5205e-01, -1.0665e+00,  1.8351e+00,  2.0743e+00,  4.5602e-02,\n",
            "        -4.9998e-01,  1.6442e+00, -2.9042e+00, -3.1741e+00, -1.1054e+00,\n",
            "        -1.4783e+00, -2.5162e-01, -9.0606e-01, -3.0062e-01,  1.7163e+00,\n",
            "         9.4308e-01, -4.8473e-01, -2.2694e-01, -1.2705e+00,  6.7764e-01,\n",
            "        -8.7626e-01, -2.7597e-01, -2.6755e-01,  1.1773e+00, -8.9875e-01,\n",
            "         7.7378e-01, -5.1233e-02, -5.6830e-01, -1.0455e+00,  4.3468e-01,\n",
            "        -1.1506e+00,  1.6511e+00,  7.0269e-01,  1.7373e-01, -4.3396e-01,\n",
            "        -6.2711e-01,  1.5180e-01,  1.0864e+00,  1.8670e+00,  1.6086e-01,\n",
            "         9.0100e-01, -3.7886e-01, -4.6470e-01, -2.9682e-01,  1.0536e-01,\n",
            "         5.3806e-01, -4.3933e-01, -2.9691e-01, -1.3388e+00,  1.3717e+00,\n",
            "        -5.9742e-01,  1.3767e-01, -9.5470e-01, -9.0829e-02, -4.2555e-02,\n",
            "        -8.1821e-01, -6.4729e-01, -1.8928e+00, -2.4671e-01,  6.3236e-01,\n",
            "        -4.0255e-02, -2.2386e+00,  1.6132e+00,  2.6476e-01,  9.2821e-01,\n",
            "        -9.8056e-01, -1.3228e+00, -2.4123e+00,  5.0638e-01,  2.4968e-01,\n",
            "        -1.0184e+00, -2.0690e+00,  2.9613e-01, -8.8604e-01, -1.0673e-01,\n",
            "        -9.6900e-01, -9.3478e-01, -1.8142e+00, -7.7374e-01,  7.3813e-01,\n",
            "         1.8340e+00, -8.4287e-01,  5.9865e-01, -2.5085e-01,  3.3784e-01,\n",
            "        -3.3770e-01,  6.1140e-01,  9.2113e-01, -1.4030e+00,  8.0839e-02,\n",
            "         1.1347e+00, -3.0054e-01, -1.5174e-01,  1.8207e+00,  5.4327e-01,\n",
            "        -1.7437e+00, -5.4589e-02, -3.4094e-01, -3.8866e-01, -1.1095e+00,\n",
            "        -4.2742e-02,  1.4997e+00,  2.5515e-01, -1.4043e+00,  1.0781e+00,\n",
            "        -4.6035e-01, -1.1425e+00,  1.0899e+00,  1.3481e+00, -9.1668e-03,\n",
            "         1.0043e+00,  5.4964e-01, -9.6666e-01, -1.8959e+00, -6.5251e-01,\n",
            "         1.3750e-01, -2.2022e-01,  3.8825e-01,  3.7908e-01,  2.0086e+00,\n",
            "         7.8876e-01, -1.2727e+00, -7.6596e-01,  3.1592e-02, -1.8747e+00,\n",
            "        -3.6472e-01,  1.5409e-01, -1.1440e+00, -3.0734e-01, -4.4900e-01,\n",
            "        -8.7200e-01, -6.2611e-01,  6.6559e-01,  5.3142e-01, -7.8436e-01,\n",
            "         2.9890e-01, -5.4281e-01, -1.4088e-01, -6.2939e-01, -2.4526e+00,\n",
            "         1.2114e-01,  4.4563e-02, -2.0857e-01, -6.5168e-01,  1.7465e-01,\n",
            "        -1.6930e+00, -1.1469e+00,  6.1809e-01, -4.2229e-01,  5.5593e-01,\n",
            "         1.2029e+00,  1.3552e+00, -5.8942e-01, -1.9869e+00, -5.5030e-01,\n",
            "         1.4304e+00, -4.2809e-01,  1.4093e+00,  2.1956e-01, -1.2559e+00,\n",
            "        -7.9999e-01, -2.4657e+00, -1.9283e+00,  6.5797e-01, -2.9893e-01,\n",
            "         1.2076e+00, -2.3267e-01,  3.4232e-01, -6.2672e-01, -2.0045e+00,\n",
            "         1.2806e+00, -8.8566e-01,  7.2189e-01, -1.4071e+00, -7.6358e-02,\n",
            "         1.0753e+00, -6.2675e-01, -1.2375e+00, -8.4089e-01, -2.0411e+00,\n",
            "         2.4061e+00,  6.8927e-01,  1.7164e+00,  2.0905e-01, -2.2672e+00,\n",
            "         3.5710e-01, -1.4704e+00, -7.3330e-01, -2.1373e-01, -6.1976e-01,\n",
            "        -1.0985e+00, -6.5321e-01,  5.9521e-01,  1.7037e+00, -1.6328e+00,\n",
            "        -1.7268e+00, -1.4117e+00,  1.6116e-01, -8.1978e-01,  9.1868e-01,\n",
            "         5.1739e-01, -8.3589e-01, -1.9510e-01, -8.6959e-01,  7.0927e-01,\n",
            "         4.0172e-01,  1.9946e-01,  1.4837e+00,  2.3003e-02, -3.4263e-01,\n",
            "         3.9280e-01,  1.2721e+00,  3.1376e-02,  2.1708e+00, -1.4610e+00,\n",
            "        -5.1266e-01,  1.2925e+00, -1.2990e-01,  1.1268e+00, -8.5902e-01,\n",
            "         4.2971e-01, -8.5749e-01,  1.4149e+00,  5.3662e-01, -1.0981e+00,\n",
            "         1.8765e+00, -4.9658e-01, -8.5043e-01,  1.8259e-01, -3.5350e-01,\n",
            "        -1.0703e+00,  6.1738e-01, -9.2604e-01, -1.2516e+00, -1.5955e-02,\n",
            "         1.0825e+00,  1.3954e+00,  3.5666e-01,  4.3343e-01, -4.0746e-01,\n",
            "        -1.6377e-01, -2.5026e-01,  3.8456e-01, -7.2378e-01, -1.8020e+00,\n",
            "         1.1617e+00, -9.9593e-01, -5.4424e-01,  1.5445e+00,  2.6348e-03,\n",
            "         3.6610e-01,  4.4268e-01, -4.4229e-01, -9.6554e-01, -1.6608e+00,\n",
            "        -1.1876e+00,  1.0066e+00,  1.8579e-01, -5.9815e-01,  1.6224e+00,\n",
            "         6.2739e-01,  7.4440e-01,  5.8981e-02, -6.4021e-01, -1.3134e-01,\n",
            "        -9.9744e-01,  3.2920e-01,  7.3204e-02, -1.0524e-01,  3.9002e-01,\n",
            "         1.2264e-01,  3.4863e-01, -5.1487e-01,  5.5934e-01,  4.0601e-01,\n",
            "         1.2576e+00,  9.2228e-01,  9.5277e-01, -7.5853e-01, -1.8203e+00,\n",
            "         5.7944e-01, -4.2228e-01, -2.4540e-01,  3.4153e-01, -5.2215e-01,\n",
            "        -1.2438e+00, -1.6408e-02, -6.4391e-01, -1.2807e-01, -7.1120e-01,\n",
            "         6.9201e-01, -9.7707e-01,  2.5133e-01,  9.1821e-01, -1.1986e+00,\n",
            "         1.3705e-01,  3.6476e-01,  1.4903e+00, -5.4502e-01,  9.8126e-01,\n",
            "        -1.7272e+00,  1.2406e+00,  1.4589e+00, -2.0405e+00, -7.8213e-01,\n",
            "         4.7152e-01,  1.1190e-01,  1.0171e-01, -3.5130e-01, -2.3079e-02,\n",
            "        -6.9934e-01,  1.4883e+00,  1.4490e+00,  5.6058e-01,  2.1381e-01,\n",
            "        -1.6871e-01,  2.6232e-01, -6.3594e-01, -1.0914e+00, -6.0212e-01,\n",
            "        -1.5843e-01, -7.6149e-01,  2.5150e-01,  4.7544e-01, -8.3378e-01,\n",
            "        -1.0244e+00,  1.2396e+00, -2.5680e-01, -1.7738e+00, -1.0635e+00,\n",
            "         1.0255e+00,  6.2105e-01, -2.9666e-01, -9.1746e-01,  1.1207e-01,\n",
            "         1.2692e+00, -9.9995e-01,  3.7366e-01,  9.6227e-01,  2.4931e-01,\n",
            "         1.4070e+00,  3.9828e-01,  8.2565e-01,  1.0696e+00, -9.0388e-01,\n",
            "         4.3047e-01,  9.3044e-01,  4.8975e-01,  2.6349e-01, -7.5032e-01,\n",
            "         4.8812e-01,  7.9048e-01,  8.2117e-01, -1.1162e+00, -7.0333e-01,\n",
            "         9.1183e-01,  1.4366e+00,  3.0501e-01,  5.6827e-01,  1.2412e+00,\n",
            "         1.7649e+00,  2.2314e+00,  9.4972e-01, -4.3466e-02, -7.1995e-01,\n",
            "        -2.4791e+00, -1.0193e+00, -5.4332e-01, -1.9494e-01,  2.9626e-01,\n",
            "        -2.4651e+00,  8.5684e-01,  2.4762e-01,  8.1744e-01,  1.1150e+00,\n",
            "         1.3004e+00,  7.4625e-01, -8.0358e-01, -8.6731e-01,  6.2943e-02,\n",
            "        -5.0410e-02, -2.5111e-01, -2.8504e-01,  1.9834e+00, -6.3579e-01,\n",
            "        -6.8068e-01, -7.0831e-01, -2.0852e-01,  1.7195e+00,  9.0390e-02,\n",
            "         6.0350e-01,  1.8241e+00, -6.4915e-01, -6.4952e-01, -1.7040e-01,\n",
            "         1.6716e+00,  7.7619e-01, -1.0657e+00, -5.1469e-01,  8.0373e-01,\n",
            "         3.7579e-01,  3.8708e-02, -8.9564e-01, -5.3077e-01,  1.5121e-02,\n",
            "         1.6577e+00, -1.0567e+00, -1.4623e-01, -8.5986e-01,  5.7659e-01,\n",
            "        -3.9589e-01,  1.3116e+00, -1.6852e+00, -3.1523e-01,  2.5590e-01,\n",
            "         1.4683e+00,  1.5338e+00, -1.2871e+00,  1.4033e+00, -1.0781e+00,\n",
            "         3.6984e-02,  7.4822e-01, -1.4884e+00,  5.2267e-01, -8.4125e-01,\n",
            "         1.4008e+00,  1.0512e+00,  1.4458e-01,  3.1265e-01,  5.1826e-01,\n",
            "        -8.5611e-01, -4.5403e-01,  7.6252e-01,  1.6912e+00,  3.0242e-01,\n",
            "         1.6232e+00, -1.2398e-01,  1.3483e+00, -6.4563e-01,  1.5494e+00,\n",
            "         4.7354e-01,  3.6990e-01,  1.2322e+00,  1.3049e+00, -7.4874e-01,\n",
            "         3.1128e+00,  2.8643e-01,  1.9510e-01,  1.7531e-01,  3.9690e-01,\n",
            "         3.1729e-01,  4.2081e-01, -7.5234e-01,  3.0505e-01,  2.5734e-01,\n",
            "         6.4562e-01,  8.9111e-01, -2.7299e-01, -2.7334e-01,  7.7897e-01,\n",
            "        -1.0739e+00, -2.3059e+00,  5.9118e-01, -7.3721e-01, -6.0196e-01,\n",
            "         3.5136e-01,  7.5251e-01,  4.9453e-01, -1.2798e+00,  1.6985e+00,\n",
            "        -1.7498e-01, -8.6017e-01,  1.2201e+00,  8.6056e-01,  4.5121e-01,\n",
            "         4.5601e-01, -2.0112e+00, -6.8305e-01,  1.6860e+00, -1.3390e-01,\n",
            "         8.3009e-01,  1.0633e+00,  9.3571e-01,  7.5652e-01, -4.7176e-01,\n",
            "        -3.6761e-01,  1.1173e+00, -3.4150e-01,  7.0833e-01, -2.5214e-01,\n",
            "         1.5350e+00,  7.0057e-01, -7.7882e-01, -6.5248e-01,  2.8478e-02,\n",
            "        -2.5451e-01,  6.6650e-01, -1.8765e+00, -1.4560e+00, -8.6991e-02,\n",
            "        -1.0112e+00, -4.8725e-01, -1.2981e-01,  2.4843e-01,  3.6272e-01,\n",
            "         1.2315e+00, -6.5443e-01, -3.1298e-01,  6.9195e-01, -1.2214e+00,\n",
            "         1.9103e+00, -5.2442e-02,  1.2186e+00, -4.0984e-02,  1.4872e+00,\n",
            "         8.6292e-01, -1.1929e+00,  1.2588e+00, -6.5285e-01,  5.6553e-01,\n",
            "        -1.5266e+00, -1.3369e+00,  1.1448e-01,  2.5376e-01,  4.9914e-01,\n",
            "         2.1722e-01,  1.6742e+00, -8.9108e-01,  6.2537e-01,  3.4789e-01,\n",
            "         2.6616e-02, -2.7532e-01, -5.6789e-01, -3.6334e-01, -1.5225e+00,\n",
            "         1.0099e+00, -6.8843e-01,  4.5611e-01,  1.3778e+00,  1.1520e+00,\n",
            "         1.7766e-01, -9.7739e-01,  6.8334e-02, -4.5191e-01,  5.0402e-01,\n",
            "         5.2267e-01, -3.5698e-01,  5.8120e-02,  1.0443e+00, -4.8901e-01,\n",
            "        -2.2606e+00, -7.0521e-01, -1.7657e+00,  7.3118e-01, -4.5196e-01,\n",
            "         1.3356e+00,  9.7283e-01, -4.5193e-01, -1.1189e+00,  4.1351e-01,\n",
            "        -3.2140e-01,  1.1560e+00,  1.3476e+00,  2.4154e-01,  1.7936e-01,\n",
            "        -2.0880e-01,  4.6926e-01,  3.0017e-01,  1.7119e+00,  6.2258e-01,\n",
            "         1.1722e+00, -7.8353e-01,  1.8342e+00, -1.5090e+00,  1.8616e-01,\n",
            "        -2.7735e-01, -1.4079e+00, -1.1785e+00, -1.1501e+00,  6.1230e-01,\n",
            "        -1.5211e+00, -3.7680e-01, -1.0150e+00, -2.7755e+00,  1.5875e+00,\n",
            "         8.5356e-01,  1.4166e+00,  4.1033e-01,  1.6185e-01, -6.4856e-01,\n",
            "         9.6311e-01,  1.3799e+00, -3.4765e-01, -6.5008e-01, -6.4443e-01,\n",
            "         7.4587e-01,  2.5247e+00,  8.6834e-01, -3.4349e-01, -1.0345e+00,\n",
            "         2.8014e-01,  2.1621e+00,  9.8085e-01, -1.4765e+00, -1.1368e-01,\n",
            "         8.3120e-01, -7.1744e-02,  2.4110e-01,  2.4267e-01,  1.4229e+00,\n",
            "         2.0002e-01, -1.8577e+00, -3.9919e-01,  3.6929e-02, -1.2566e+00,\n",
            "        -4.6089e-01, -1.3044e+00, -1.2874e-01, -1.6080e+00, -8.1635e-01,\n",
            "         8.6891e-01, -1.6002e+00, -2.6051e-01, -7.8736e-01, -1.5007e+00,\n",
            "        -3.8767e-01,  1.4242e+00,  6.5536e-01,  4.3984e-01,  4.3536e-01,\n",
            "         1.1955e+00,  7.0584e-01,  7.5754e-02,  2.5967e-01,  5.2447e-01,\n",
            "        -9.2214e-01,  4.1959e-01, -1.0490e+00,  2.7289e-01, -1.1613e+00,\n",
            "         1.2453e-01, -1.4236e+00,  7.7055e-01, -1.3323e+00,  1.6454e+00,\n",
            "         1.5500e+00,  5.8388e-01, -4.5448e-01,  7.7142e-01, -4.8856e-01,\n",
            "        -1.0496e+00, -9.0243e-01, -2.4570e-01,  5.8340e-01,  2.2674e+00,\n",
            "        -1.7853e-01,  9.4089e-01, -1.2760e+00,  2.3045e-03,  1.4382e+00,\n",
            "         1.8315e+00,  4.2640e-01,  2.6071e-01,  1.3409e+00, -5.3242e-01,\n",
            "         1.3279e+00,  5.2165e-01,  7.9042e-02,  5.9274e-01, -9.9494e-01,\n",
            "         1.4860e+00,  6.6110e-01,  1.1772e-01, -1.2501e+00,  1.0685e+00,\n",
            "         1.6349e+00,  1.3184e-01, -1.5346e+00,  2.0896e+00,  4.4542e-02,\n",
            "        -5.7348e-01, -2.7060e-02,  3.2838e-01,  3.3026e-01,  2.2769e-01,\n",
            "        -1.1302e+00, -1.9339e-01, -1.4456e-01, -1.0464e+00, -1.1084e+00,\n",
            "        -6.5960e-01,  9.4079e-01,  3.2102e-02, -1.6066e+00, -2.2600e-02,\n",
            "        -1.1877e+00,  2.0725e+00, -6.4808e-01,  1.9323e+00, -2.8451e-01,\n",
            "        -1.2166e+00, -3.8155e-01, -6.7514e-01, -1.1892e+00, -6.0265e-01,\n",
            "        -4.6015e-01,  8.0101e-01,  1.6756e+00,  5.8805e-01, -7.3430e-01,\n",
            "         5.2018e-01, -1.5243e+00, -1.7107e-01,  2.7398e-01, -5.3467e-01,\n",
            "        -6.5693e-01,  1.2420e+00, -5.9053e-01], grad_fn=<SelectBackward0>) \n",
            "\n",
            "Attention M:  tensor([1.0000e+00, 5.4529e-28, 1.7899e-27, 4.0387e-29, 4.1976e-27, 2.6853e-41,\n",
            "        1.4013e-44, 2.8853e-42, 1.1494e-40, 2.3472e-42, 3.1263e-42, 8.7161e-43,\n",
            "        5.6052e-45, 2.3304e-42, 4.6243e-44, 1.4100e-40, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "emb = Embedding()\n",
        "embeds = emb(input_ids, segment_ids)\n",
        "\n",
        "attenM = get_attn_pad_mask(input_ids, input_ids)\n",
        "\n",
        "SDPA= ScaledDotProductAttention()(embeds, embeds, embeds, attenM)\n",
        "\n",
        "S, A = SDPA\n",
        "\n",
        "# print('Masks',masks[0][0])\n",
        "# print()\n",
        "print('Scores: ', S[0][0],'\\n\\nAttention M: ', A[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hUX_eM_E1B8p"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
        "\n",
        "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
        "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
        "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs_xOAZy3pay",
        "outputId": "e77fa002-72b7-4904-e0b6-d154af0d3d3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0612, 0.0597, 0.0543, 0.0653, 0.0512, 0.0696, 0.0634, 0.0409, 0.0713,\n",
              "         0.0703, 0.0843, 0.0620, 0.0746, 0.0503, 0.0570, 0.0644, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0603, 0.0598, 0.0833, 0.0537, 0.1107, 0.0544, 0.0668, 0.0541, 0.0368,\n",
              "         0.0404, 0.0372, 0.0450, 0.0492, 0.0462, 0.0776, 0.1246, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0416, 0.0449, 0.0364, 0.0736, 0.1113, 0.0528, 0.0617, 0.0607, 0.0888,\n",
              "         0.0546, 0.0486, 0.0372, 0.0553, 0.0584, 0.0602, 0.1140, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0375, 0.0524, 0.0597, 0.0517, 0.0840, 0.0757, 0.0806, 0.0734, 0.0482,\n",
              "         0.0595, 0.0537, 0.0488, 0.0541, 0.0618, 0.0549, 0.1040, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0784, 0.0455, 0.0638, 0.0741, 0.1010, 0.0843, 0.0451, 0.0606, 0.0373,\n",
              "         0.0631, 0.0469, 0.0467, 0.0493, 0.0504, 0.0481, 0.1055, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0713, 0.0386, 0.0874, 0.0382, 0.0550, 0.0584, 0.0445, 0.0615, 0.0492,\n",
              "         0.0809, 0.0534, 0.0809, 0.0758, 0.0556, 0.0533, 0.0958, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0887, 0.0705, 0.0483, 0.0391, 0.1317, 0.0575, 0.0802, 0.0646, 0.0268,\n",
              "         0.0681, 0.0429, 0.0600, 0.0671, 0.0519, 0.0365, 0.0660, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0761, 0.0354, 0.0587, 0.0447, 0.0563, 0.0973, 0.0903, 0.0811, 0.0438,\n",
              "         0.0963, 0.0536, 0.0602, 0.0540, 0.0457, 0.0509, 0.0554, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0453, 0.0646, 0.0523, 0.0452, 0.0754, 0.0740, 0.0803, 0.0598, 0.0581,\n",
              "         0.0716, 0.0657, 0.0344, 0.0965, 0.0524, 0.0664, 0.0581, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0853, 0.0454, 0.0677, 0.0582, 0.0473, 0.0396, 0.0503, 0.0479, 0.0617,\n",
              "         0.0724, 0.0743, 0.0729, 0.1097, 0.0547, 0.0639, 0.0490, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0472, 0.0480, 0.0890, 0.0348, 0.0938, 0.0643, 0.0358, 0.0587, 0.0428,\n",
              "         0.0773, 0.0424, 0.0854, 0.0892, 0.0713, 0.0470, 0.0731, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0578, 0.0443, 0.0893, 0.0374, 0.0747, 0.0614, 0.0598, 0.0660, 0.0357,\n",
              "         0.0884, 0.0309, 0.0906, 0.0649, 0.0882, 0.0598, 0.0508, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.1029, 0.0505, 0.0864, 0.0578, 0.0812, 0.0631, 0.0516, 0.0502, 0.0507,\n",
              "         0.0674, 0.0367, 0.0519, 0.0681, 0.0468, 0.0615, 0.0732, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0825, 0.0598, 0.0468, 0.0506, 0.0720, 0.0911, 0.0406, 0.0563, 0.0316,\n",
              "         0.0565, 0.0526, 0.0748, 0.0764, 0.0412, 0.0808, 0.0863, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0659, 0.0535, 0.0627, 0.0344, 0.0842, 0.0678, 0.0521, 0.0624, 0.0563,\n",
              "         0.0906, 0.0354, 0.0684, 0.0869, 0.0537, 0.0670, 0.0589, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0635, 0.0465, 0.0569, 0.0348, 0.0869, 0.0925, 0.0387, 0.0632, 0.0551,\n",
              "         0.1054, 0.0362, 0.0495, 0.0913, 0.0552, 0.0582, 0.0660, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0673, 0.0447, 0.0502, 0.0592, 0.0492, 0.0934, 0.0649, 0.0812, 0.0570,\n",
              "         0.0886, 0.0587, 0.0495, 0.0626, 0.0449, 0.0494, 0.0791, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0914, 0.0745, 0.0631, 0.0818, 0.0717, 0.0912, 0.0514, 0.0489, 0.0552,\n",
              "         0.0580, 0.0454, 0.0412, 0.0469, 0.0521, 0.0378, 0.0893, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0630, 0.0497, 0.0480, 0.0694, 0.0622, 0.0752, 0.0907, 0.0687, 0.0475,\n",
              "         0.0807, 0.0765, 0.0497, 0.0443, 0.0580, 0.0457, 0.0707, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0854, 0.0586, 0.0761, 0.0613, 0.0884, 0.0854, 0.0712, 0.0578, 0.0422,\n",
              "         0.0605, 0.0595, 0.0489, 0.0349, 0.0444, 0.0411, 0.0845, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0959, 0.0775, 0.0639, 0.0717, 0.0711, 0.0540, 0.0676, 0.0636, 0.0325,\n",
              "         0.0523, 0.0480, 0.0795, 0.0619, 0.0414, 0.0507, 0.0685, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0854, 0.0553, 0.0588, 0.0940, 0.0845, 0.0669, 0.0508, 0.0658, 0.0479,\n",
              "         0.0541, 0.0552, 0.0609, 0.0497, 0.0404, 0.0360, 0.0944, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0890, 0.0410, 0.0397, 0.0791, 0.0700, 0.0961, 0.0499, 0.0719, 0.0464,\n",
              "         0.0752, 0.0601, 0.0568, 0.0525, 0.0586, 0.0302, 0.0834, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0982, 0.0652, 0.0558, 0.0858, 0.0969, 0.0832, 0.0496, 0.0504, 0.0479,\n",
              "         0.0551, 0.0546, 0.0319, 0.0467, 0.0404, 0.0365, 0.1020, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0665, 0.0600, 0.0465, 0.0760, 0.0559, 0.0943, 0.0553, 0.0711, 0.0702,\n",
              "         0.0659, 0.0468, 0.0408, 0.0586, 0.0587, 0.0416, 0.0917, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.1241, 0.0627, 0.0693, 0.0751, 0.0738, 0.0688, 0.0510, 0.0564, 0.0521,\n",
              "         0.0537, 0.0637, 0.0442, 0.0508, 0.0375, 0.0465, 0.0705, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0595, 0.0486, 0.0764, 0.0721, 0.0744, 0.0719, 0.0672, 0.0707, 0.0543,\n",
              "         0.0539, 0.0716, 0.0624, 0.0441, 0.0437, 0.0479, 0.0810, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0656, 0.0509, 0.0566, 0.0514, 0.0757, 0.0867, 0.0648, 0.0622, 0.0469,\n",
              "         0.0661, 0.0511, 0.0567, 0.0664, 0.0619, 0.0462, 0.0908, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0946, 0.0625, 0.0596, 0.0750, 0.0542, 0.0835, 0.0587, 0.0685, 0.0589,\n",
              "         0.0742, 0.0554, 0.0615, 0.0604, 0.0476, 0.0340, 0.0514, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0788, 0.0935, 0.0587, 0.0677, 0.0704, 0.0841, 0.0665, 0.0502, 0.0501,\n",
              "         0.0546, 0.0410, 0.0642, 0.0479, 0.0440, 0.0492, 0.0792, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000]], grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb = Embedding()\n",
        "embeds = emb(input_ids, segment_ids)\n",
        "\n",
        "attenM = get_attn_pad_mask(input_ids, input_ids)\n",
        "\n",
        "MHA= MultiHeadAttention()(embeds, embeds, embeds, attenM)\n",
        "\n",
        "Output, A = MHA\n",
        "\n",
        "A[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_GQFL_Va4N4Y"
      },
      "outputs": [],
      "source": [
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
        "        return self.fc2(gelu(self.fc1(x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RgmfjTqw4Qnw"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
        "        return enc_outputs, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OZ0TJ84W4SZw"
      },
      "outputs": [],
      "source": [
        "class BERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embedding = Embedding()\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.activ1 = nn.Tanh()\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.activ2 = gelu\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        # decoder is shared with embedding layer\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        n_vocab, n_dim = embed_weight.size()\n",
        "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
        "        self.decoder.weight = embed_weight\n",
        "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
        "        for layer in self.layers:\n",
        "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
        "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
        "        # it will be decided by first token(CLS)\n",
        "        h_pooled = self.activ1(self.fc(output[:, 0])) # [batch_size, d_model]\n",
        "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2]\n",
        "\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
        "        # get masked position from final output of transformer.\n",
        "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
        "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
        "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
        "\n",
        "        return logits_lm, logits_clsf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UAG3SEP4UbU",
        "outputId": "bc6f202f-df37-4fac-843c-fb86bdb777b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0010 cost = 25.630148\n"
          ]
        }
      ],
      "source": [
        "model = BERT()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "batch = make_batch()\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
        "\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
        "    loss_lm = (loss_lm.float()).mean()\n",
        "    loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
        "    loss = loss_lm + loss_clsf\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD3K8T6B4YJp",
        "outputId": "e057405b-1f78-431c-fa71-032a738fb848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, how are you? I am Romeo.\n",
            "Hello, Romeo My name is Juliet. Nice to meet you.\n",
            "Nice meet you too. How are you today?\n",
            "Great. My baseball team won the competition.\n",
            "Oh Congratulations, Juliet\n",
            "Thanks you Romeo\n",
            "['[CLS]', 'oh', 'congratulations', 'juliet', '[SEP]', 'nice', 'meet', '[MASK]', 'too', 'how', '[MASK]', 'you', 'today', '[SEP]']\n",
            "masked tokens list :  [22, 24]\n",
            "predict masked tokens list :  []\n",
            "[0 0 0 0 0]\n",
            "isNext :  False\n",
            "predict isNext :  False\n"
          ]
        }
      ],
      "source": [
        "# Predict mask tokens ans isNext\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
        "print(text)\n",
        "print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
        "\n",
        "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
        "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
        "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
        "print(logits_lm)\n",
        "\n",
        "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
        "print('isNext : ', True if isNext else False)\n",
        "print('predict isNext : ',True if logits_clsf else False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf97uJJS4grJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Bidirectional encoder representation from Transformers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
